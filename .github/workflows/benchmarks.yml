name: Performance Benchmarks

on:
  # Trigger after CI workflow completes (will have artifacts)
  workflow_run:
    workflows: ["CI"]
    types:
      - completed
    branches: [ master, main, perf-figures-improvements ]
  push:
    branches: [ master, main, perf-figures-improvements ]
  pull_request:
    branches: [ master, main, perf-figures-improvements ]
  workflow_dispatch:  # Allow manual trigger
  schedule:
    # Run weekly on Monday at 00:00 UTC to track performance over time
    - cron: '0 0 * * 1'

jobs:
  # Job 1: Collect performance data from CI artifacts or run locally
  collect-performance-data:
    name: Collect Performance Data
    runs-on: ubuntu-latest
    
    steps:
    - uses: actions/checkout@v4
      with:
        fetch-depth: 0
    
    # Try to download artifacts from CI workflow
    - name: Download CI performance artifacts
      if: github.event_name == 'workflow_run'
      uses: actions/download-artifact@v4
      with:
        pattern: perf-*
        path: artifacts/
        merge-multiple: false
        run-id: ${{ github.event.workflow_run.id }}
      continue-on-error: true
    
    # If no artifacts available, run benchmarks locally
    - name: Check if artifacts were downloaded
      id: check_artifacts
      run: |
        if [ -d "artifacts" ] && [ "$(find artifacts -name '*.csv' | wc -l)" -gt 0 ]; then
          echo "has_artifacts=true" >> $GITHUB_OUTPUT
          echo "‚úÖ Found CI artifacts, will use them"
        else
          echo "has_artifacts=false" >> $GITHUB_OUTPUT
          echo "‚ö†Ô∏è  No CI artifacts found, will run benchmarks locally"
        fi
    
    - name: Install build dependencies (if running locally)
      if: steps.check_artifacts.outputs.has_artifacts == 'false'
      run: |
        sudo apt-get update
        sudo apt-get install -y build-essential cmake
    
    - name: Build and run benchmarks locally (fallback)
      if: steps.check_artifacts.outputs.has_artifacts == 'false'
      run: |
        # Build tests
        cmake -B build -DCMAKE_BUILD_TYPE=Release -DFASTCOND_BUILD_TESTS=ON
        cmake --build build -j$(nproc)
        
        # Create artifacts directory structure
        mkdir -p artifacts/perf-Linux-gcc
        
        # Run benchmarks with local build
        ./scripts/run_performance_benchmarks.sh build artifacts/perf-Linux-gcc/performance-results.csv
        
        echo "‚úÖ Local benchmarks complete"
    
    - name: Merge all performance data
      run: |
        # Combine all CSV files into one
        mkdir -p combined
        
        # Write header
        echo "platform,os_version,test,variant,threads,param,iterations,elapsed_sec,throughput" > combined/all-performance-data.csv
        
        # Append all data (skip headers from individual files)
        find artifacts -name 'performance-results.csv' -exec tail -n +2 {} \; >> combined/all-performance-data.csv
        
        echo "‚úÖ Merged performance data:"
        wc -l combined/all-performance-data.csv
        head -20 combined/all-performance-data.csv
    
    - name: Upload combined performance data
      uses: actions/upload-artifact@v4
      with:
        name: combined-performance-data
        path: combined/all-performance-data.csv
        retention-days: 90

  # Job 2: Analyze performance data and generate reports
  analyze-performance:
    name: Analyze Performance Results
    runs-on: ubuntu-latest
    needs: collect-performance-data
    
    steps:
    - uses: actions/checkout@v4
    
    - name: Install uv
      uses: astral-sh/setup-uv@v5
      with:
        enable-cache: true
    
    - name: Install Python dependencies
      run: |
        cd scripts
        uv sync
    
    - name: Download combined performance data
      uses: actions/download-artifact@v4
      with:
        name: combined-performance-data
        path: data/
    
    - name: Run performance analysis
      run: |
        mkdir -p docs
        cd scripts
        uv run python analyze_performance.py ../data/all-performance-data.csv --output-dir ../docs
    
    - name: Generate index page
      run: |
        python3 << 'PYSCRIPT'
        import datetime
        html = f"""<!DOCTYPE html>
        <html lang="en">
        <head>
            <meta charset="UTF-8">
            <meta name="viewport" content="width=device-width, initial-scale=1.0">
            <title>fastcond Performance Results</title>
            <style>
                body {{
                    font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, sans-serif;
                    max-width: 1200px;
                    margin: 0 auto;
                    padding: 20px;
                    line-height: 1.6;
                }}
                h1 {{ color: #2c3e50; border-bottom: 3px solid #3498db; padding-bottom: 10px; }}
                h2 {{ color: #34495e; margin-top: 30px; }}
                img {{ max-width: 100%; height: auto; margin: 20px 0; box-shadow: 0 2px 8px rgba(0,0,0,0.1); }}
                .chart-container {{ margin: 30px 0; }}
                table {{ border-collapse: collapse; width: 100%; margin: 20px 0; }}
                th, td {{ border: 1px solid #ddd; padding: 12px; text-align: left; }}
                th {{ background-color: #3498db; color: white; }}
                tr:nth-child(even) {{ background-color: #f2f2f2; }}
                .footer {{ margin-top: 50px; padding-top: 20px; border-top: 1px solid #ddd; color: #7f8c8d; }}
            </style>
        </head>
        <body>
            <h1>üöÄ fastcond Performance Benchmarks</h1>
            <p>Cross-platform performance comparison of fastcond synchronization primitives vs native implementations.</p>
            
            <h2>üìä Performance Comparison Charts</h2>
            <div class="chart-container">
                <h3>Throughput Comparison</h3>
                <img src="performance-comparison.png" alt="Performance Comparison Across Platforms">
            </div>
            
            <div class="chart-container">
                <h3>Speedup vs Native</h3>
                <img src="speedup-comparison.png" alt="Speedup Comparison">
            </div>
            
            <h2>üìà Detailed Results</h2>
            <p>See <a href="performance-comparison.md">performance-comparison.md</a> for detailed benchmark results.</p>
            <p>Download raw data: <a href="performance-summary.json">performance-summary.json</a></p>
            
            <div class="footer">
                <p>Generated: {datetime.datetime.utcnow().strftime("%Y-%m-%d %H:%M:%S UTC")}</p>
                <p>Repository: <a href="https://github.com/kristjanvalur/fastcond">kristjanvalur/fastcond</a></p>
            </div>
        </body>
        </html>
        """
        with open('docs/index.html', 'w') as f:
            f.write(html)
        PYSCRIPT
    
    - name: Upload analysis results
      uses: actions/upload-artifact@v4
      with:
        name: performance-analysis
        path: |
          docs/performance-comparison.md
          docs/performance-summary.json
          docs/performance-comparison.png
          docs/speedup-comparison.png
          docs/index.html
        retention-days: 90
    
    - name: Deploy to GitHub Pages
      if: github.ref == 'refs/heads/master' && github.event_name == 'push'
      uses: peaceiris/actions-gh-pages@v4
      with:
        github_token: ${{ secrets.GITHUB_TOKEN }}
        publish_dir: ./docs
        keep_files: true
        destination_dir: performance

  # Job 3: Original benchmark job (kept for backwards compatibility)
  benchmark:
    name: Run Performance Benchmarks
    runs-on: ubuntu-latest
    
    steps:
    - uses: actions/checkout@v4
      with:
        fetch-depth: 0  # Fetch all history for performance tracking
    
    - name: Install uv
      uses: astral-sh/setup-uv@v5
      with:
        enable-cache: true
    
    - name: Install build dependencies
      run: |
        sudo apt-get update
        sudo apt-get install -y build-essential cmake
    
    - name: Install Python dependencies
      run: |
        cd scripts
        uv sync
    
    - name: Build tests (Release mode for accurate benchmarks)
      run: |
        cd test
        make clean
        make all CFLAGS="-O3 -DNDEBUG"
    
    - name: Run benchmark suite
      run: |
        echo "üîç Before benchmark generation - docs/ contents:"
        ls -la docs/ || echo "docs/ directory not found"
        echo "üöÄ Running benchmark suite..."
        BUILD_DIR=test ./scripts/run_benchmarks.sh
        echo "‚úÖ After benchmark generation - docs/ contents:"
        ls -la docs/
        echo "üìä Generated files details:"
        find docs/ -name "*.json" -o -name "*.html" -o -name "*.png" | xargs ls -la 2>/dev/null || echo "No generated files found"
    
    - name: Run GIL fairness analysis
      run: |
        echo "üîç Before GIL analysis - docs/ contents:"
        ls -la docs/
        # Ensure GIL test executables are available
        cd test && ls -la gil_test_* || echo "No GIL tests found"
        cd ..
        echo "üé≠ Running GIL fairness analysis..."
        BUILD_DIR=test ./scripts/run_gil_fairness_analysis.sh
        echo "‚úÖ After GIL analysis - docs/ contents:"
        ls -la docs/
        echo "üìä All generated files summary:"
        find docs/ -name "*.json" -o -name "*.html" -o -name "*.png" | xargs ls -la 2>/dev/null || echo "No generated files found"
        
    - name: Final diagnostics before deployment
      run: |
        echo "üéØ FINAL GENERATION SUMMARY"
        echo "========================================="
        echo "üìÅ Working directory: $(pwd)"
        echo "üìä Complete docs/ directory listing:"
        ls -la docs/
        echo ""
        echo "üìà Expected files check:"
        expected_files=("index.html" "gil-fairness.html" "benchmark-results.json" "benchmark-results.md" "gil-fairness-results.json" "throughput-comparison.png" "latency-comparison.png" "gil_fairness_comparison.png")
        for file in "${expected_files[@]}"; do
          if [ -f "docs/$file" ]; then
            size=$(stat -c%s "docs/$file" 2>/dev/null || echo "unknown")
            echo "  ‚úÖ $file ($size bytes)"
          else
            echo "  ‚ùå $file (MISSING)"
          fi
        done
        echo ""
        echo "üìÇ Total file count: $(find docs/ -type f | wc -l)"
        echo "üíæ Total size: $(du -sh docs/ | cut -f1)"
        echo "========================================="
    
    - name: Upload benchmark results
      run: |
        echo "üì¶ Preparing artifacts for upload..."
        echo "üìä Files to be uploaded:"
        for file in docs/benchmark-results.json docs/benchmark-results.md docs/index.html docs/throughput-comparison.png docs/latency-comparison.png docs/gil-fairness-results.json docs/gil-fairness.html docs/gil_fairness_comparison.png; do
          if [ -f "$file" ]; then
            echo "  ‚úÖ $file ($(ls -lh "$file" | awk '{print $5}'))"
          else
            echo "  ‚ùå $file (missing)"
          fi
        done
        
    - name: Store benchmark artifacts
      uses: actions/upload-artifact@v4
      with:
        name: benchmark-results-${{ github.sha }}
        path: |
          docs/benchmark-results.json
          docs/benchmark-results.md
          docs/index.html
          docs/throughput-comparison.png
          docs/latency-comparison.png
          docs/gil-fairness-results.json
          docs/gil-fairness.html
          docs/gil_fairness_comparison.png
        retention-days: 90
    
    - name: Comment PR with results
      if: github.event_name == 'pull_request'
      uses: actions/github-script@v7
      with:
        script: |
          const fs = require('fs');
          const resultsTable = fs.readFileSync('docs/benchmark-results.md', 'utf8');
          
          // Create comment body
          const body = `## üìä Performance Benchmark Results
          
          ${resultsTable}
          
          <details>
          <summary>View Charts</summary>
          
          See the workflow artifacts for throughput and latency comparison charts.
          
          </details>
          
          ---
          *Benchmarked on: \`${{ runner.os }}\` with \`${{ runner.arch }}\`*
          *Commit: \`${{ github.sha }}\`*`;
          
          github.rest.issues.createComment({
            issue_number: context.issue.number,
            owner: context.repo.owner,
            repo: context.repo.repo,
            body: body
          });
    
    - name: Prepare files for GitHub Pages deployment
      if: |
        (github.ref == 'refs/heads/master' && github.event_name == 'push') ||
        (github.event_name == 'pull_request_target' && github.event.action == 'closed' && github.event.pull_request.merged == true)
      run: |
        echo "üöÄ Preparing GitHub Pages deployment..."
        echo "üìÅ Current working directory: $(pwd)"
        echo "üìä docs/ directory contents before deployment:"
        ls -la docs/
        echo "üîç Generated files verification:"
        echo "  index.html exists: $(test -f docs/index.html && echo 'YES' || echo 'NO')"
        echo "  gil-fairness.html exists: $(test -f docs/gil-fairness.html && echo 'YES' || echo 'NO')"
        echo "  benchmark-results.json exists: $(test -f docs/benchmark-results.json && echo 'YES' || echo 'NO')"
        echo "üìã Detailed file listing with sizes:"
        find docs/ -type f -exec ls -lh {} \; 2>/dev/null || echo "No files found in docs/"
        
        echo ""
        echo "üîß Creating deployment staging area (ignoring .gitignore)..."
        # Create a temporary staging area for deployment
        mkdir -p docs-deploy
        
        # Copy all files from docs/ to docs-deploy/, including gitignored files
        cp -r docs/* docs-deploy/ 2>/dev/null || true
        
        echo "üìÅ Deployment staging area contents:"
        ls -la docs-deploy/
        echo "üìè File count in deployment area: $(find docs-deploy/ -type f | wc -l)"
        echo "üíæ Total deployment size: $(du -sh docs-deploy/ | cut -f1)"
      
    - name: Deploy to GitHub Pages
      if: |
        (github.ref == 'refs/heads/master' && github.event_name == 'push') ||
        (github.event_name == 'pull_request_target' && github.event.action == 'closed' && github.event.pull_request.merged == true)
      uses: peaceiris/actions-gh-pages@v4
      with:
        github_token: ${{ secrets.GITHUB_TOKEN }}
        publish_dir: ./docs-deploy       # Use staging area that bypasses .gitignore
        keep_files: false               # Always replace entire content
        allow_empty_commit: true        # Force deployment even if content appears unchanged
        force_orphan: true              # Force orphan gh-pages branch (clean history)
        publish_branch: gh-pages        # Target branch
        enable_jekyll: false            # Disable Jekyll processing
        exclude_assets: '.github'       # Don't include .github folder in deployment

  compare-baseline:
    name: Compare with Baseline
    runs-on: ubuntu-latest
    if: github.event_name == 'pull_request'
    
    steps:
    - uses: actions/checkout@v4
      with:
        fetch-depth: 0
    
    - name: Install uv
      uses: astral-sh/setup-uv@v5
    
    - name: Install dependencies
      run: |
        sudo apt-get update
        sudo apt-get install -y build-essential cmake
        cd scripts
        uv sync
    
    # Run benchmarks on PR branch
    - name: Build and benchmark PR
      run: |
        cd test && make clean && make all CFLAGS="-O3 -DNDEBUG"
        cd ..
        BUILD_DIR=test ./scripts/run_benchmarks.sh
        mv docs/benchmark-results.json /tmp/pr-results.json
    
    # Checkout and run benchmarks on base branch
    - name: Checkout base branch
      run: |
        # Clean any generated files (not tracked in git)
        rm -rf docs/benchmark-results.* docs/index.html docs/*-comparison.png || true
        git fetch origin ${{ github.base_ref }}
        git checkout origin/${{ github.base_ref }}
    
    - name: Build and benchmark base
      id: base_benchmark
      continue-on-error: true
      run: |
        echo "Attempting to build and benchmark base branch..."
        if cd test && make clean && make all CFLAGS="-O3 -DNDEBUG" 2>/dev/null; then
          cd ..
          if BUILD_DIR=test ./scripts/run_benchmarks.sh 2>/dev/null; then
            mv docs/benchmark-results.json /tmp/base-results.json
            echo "base_success=true" >> $GITHUB_OUTPUT
          else
            echo "‚ö†Ô∏è Base branch benchmark execution failed - benchmark infrastructure may be incompatible"
            echo "base_success=false" >> $GITHUB_OUTPUT
          fi
        else
          echo "‚ö†Ô∏è Base branch build failed - benchmark infrastructure may be missing"
          echo "base_success=false" >> $GITHUB_OUTPUT
        fi
    
    - name: Compare results
      run: |
        # Create a robust comparison script that handles missing/incompatible base data
        cat > compare.py << 'PYEOF'
        import json
        import sys
        import os
        
        def safe_load_results(path):
            try:
                with open(path) as f:
                    data = json.load(f)
                    # Validate structure
                    if isinstance(data, list) and len(data) > 0:
                        return {r['benchmark'] + '_' + r['implementation']: r for r in data}
                    return {}
            except (FileNotFoundError, json.JSONDecodeError, KeyError) as e:
                print(f"‚ö†Ô∏è Could not load {path}: {e}")
                return {}
        
        def main():
            # Load results with error handling
            base = safe_load_results('/tmp/base-results.json')
            pr = safe_load_results('/tmp/pr-results.json')
            
            if not base:
                print("## üìä Performance Benchmarks (Base Branch Unavailable)\n")
                print("‚ö†Ô∏è **Base branch comparison unavailable** - benchmark infrastructure may be incompatible\n")
                print("### PR Branch Results Summary\n")
                
                if pr:
                    for key in sorted(pr.keys()):
                        pr_throughput = pr[key]['results']['overall'].get('throughput_items_per_sec', 0)
                        print(f"‚úÖ **{key}**: {pr_throughput:,.0f} items/sec")
                else:
                    print("‚ùå No valid benchmark results available")
                return
            
            print("## üî¨ Performance Comparison vs Base Branch\n")
            
            compared_any = False
            for key in sorted(pr.keys()):
                if key not in base:
                    print(f"üÜï **{key}**: New benchmark (no baseline)")
                    continue
                
                pr_throughput = pr[key]['results']['overall'].get('throughput_items_per_sec', 0)
                base_throughput = base[key]['results']['overall'].get('throughput_items_per_sec', 0)
                
                if base_throughput > 0:
                    change = ((pr_throughput / base_throughput) - 1) * 100
                    emoji = "üü¢" if change > 2 else "üî¥" if change < -2 else "‚ö™"
                    print(f"{emoji} **{key}**: {change:+.1f}% ({pr_throughput:,.0f} vs {base_throughput:,.0f} items/sec)")
                    compared_any = True
            
            if not compared_any:
                print("‚ö†Ô∏è No benchmarks could be compared - likely incompatible result formats")
        
        if __name__ == "__main__":
            main()
        PYEOF
        
        cd scripts
        uv run python ../compare.py > /tmp/comparison.md
        cat /tmp/comparison.md
    
    - name: Comment comparison
      uses: actions/github-script@v7
      with:
        script: |
          const fs = require('fs');
          const comparison = fs.readFileSync('/tmp/comparison.md', 'utf8');
          
          github.rest.issues.createComment({
            issue_number: context.issue.number,
            owner: context.repo.owner,
            repo: context.repo.repo,
            body: `## üî¨ Performance Regression Check\n\n${comparison}\n\n---\n*Changes > ¬±2% are highlighted*`
          });
