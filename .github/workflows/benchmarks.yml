name: Performance Benchmarksname: Performance Benchmarks



on:on:

  # Trigger after CI workflow completes (will have artifacts)  # Trigger after CI workflow completes (will have artifacts)

  workflow_run:  workflow_run:

    workflows: ["CI"]    workflows: ["CI"]

    types:    types:

      - completed      - completed

    branches: [ master, main ]    branches: [ master, main ]

  push:  push:

    branches: [ master, main ]    branches: [ master, main ]

  pull_request:  pull_request:

    branches: [ master, main ]    branches: [ master, main ]

  workflow_dispatch:  # Allow manual trigger  workflow_dispatch:  # Allow manual trigger

  schedule:  schedule:

    # Run weekly on Monday at 00:00 UTC to track performance over time    # Run weekly on Monday at 00:00 UTC to track performance over time

    - cron: '0 0 * * 1'    - cron: '0 0 * * 1'



jobs:jobs:

  # New approach: Collect performance data from CI artifacts or run locally  # Job 1: Collect performance data from CI artifacts or run locally

  collect-performance-data:  collect-performance-data:

    name: Collect Performance Data    name: Collect Performance Data

    runs-on: ubuntu-latest    runs-on: ubuntu-latest

        

    steps:    steps:

    - uses: actions/checkout@v4    - uses: actions/checkout@v4

      with:      with:

        fetch-depth: 0        fetch-depth: 0

        

    # Try to download artifacts from CI workflow    # Try to download artifacts from CI workflow

    - name: Download CI performance artifacts    - name: Download CI performance artifacts

      if: github.event_name == 'workflow_run'      if: github.event_name == 'workflow_run'

      uses: actions/download-artifact@v4      uses: actions/download-artifact@v4

      with:      with:

        pattern: perf-*        pattern: perf-*

        path: artifacts/        path: artifacts/

        merge-multiple: false        merge-multiple: false

      continue-on-error: true      continue-on-error: true

        

    # If no artifacts available, run benchmarks locally    # If no artifacts available, run benchmarks locally

    - name: Check if artifacts were downloaded    - name: Check if artifacts were downloaded

      id: check_artifacts      id: check_artifacts

      run: |      run: |

        if [ -d "artifacts" ] && [ "$(find artifacts -name '*.csv' | wc -l)" -gt 0 ]; then        if [ -d "artifacts" ] && [ "$(find artifacts -name '*.csv' | wc -l)" -gt 0 ]; then

          echo "has_artifacts=true" >> $GITHUB_OUTPUT          echo "has_artifacts=true" >> $GITHUB_OUTPUT

          echo "‚úÖ Found CI artifacts, will use them"          echo "‚úÖ Found CI artifacts, will use them"

        else        else

          echo "has_artifacts=false" >> $GITHUB_OUTPUT          echo "has_artifacts=false" >> $GITHUB_OUTPUT

          echo "‚ö†Ô∏è  No CI artifacts found, will run benchmarks locally"          echo "‚ö†Ô∏è  No CI artifacts found, will run benchmarks locally"

        fi        fi

        

    - name: Install build dependencies (if running locally)    - name: Install build dependencies (if running locally)

      if: steps.check_artifacts.outputs.has_artifacts == 'false'      if: steps.check_artifacts.outputs.has_artifacts == 'false'

      run: |      run: |

        sudo apt-get update        sudo apt-get update

        sudo apt-get install -y build-essential cmake        sudo apt-get install -y build-essential cmake

        

    - name: Build and run benchmarks locally (fallback)    - name: Build and run benchmarks locally (fallback)

      if: steps.check_artifacts.outputs.has_artifacts == 'false'      if: steps.check_artifacts.outputs.has_artifacts == 'false'

      run: |      run: |

        # Build tests        # Build tests

        cmake -B build -DCMAKE_BUILD_TYPE=Release -DFASTCOND_BUILD_TESTS=ON        cmake -B build -DCMAKE_BUILD_TYPE=Release -DFASTCOND_BUILD_TESTS=ON

        cmake --build build -j$(nproc)        cmake --build build -j$(nproc)

                

        # Create artifacts directory structure        # Create artifacts directory structure

        mkdir -p artifacts/perf-Linux-gcc        mkdir -p artifacts/perf-Linux-gcc

                mkdir -p artifacts/perf-Linux-clang

        # Run benchmarks with local build        

        ./scripts/run_performance_benchmarks.sh build artifacts/perf-Linux-gcc/performance-results.csv        # Run benchmarks with GCC build (primary)

                ./scripts/run_performance_benchmarks.sh build artifacts/perf-Linux-gcc/performance-results.csv

        echo "‚úÖ Local benchmarks complete"        

            echo "‚úÖ Local benchmarks complete"

    - name: Merge all performance data    

      run: |    - name: Merge all performance data

        # Combine all CSV files into one      run: |

        mkdir -p combined        # Combine all CSV files into one

                mkdir -p combined

        # Write header        

        echo "platform,os_version,test,variant,threads,param,iterations,elapsed_sec,throughput" > combined/all-performance-data.csv        # Write header

                echo "platform,os_version,test,variant,threads,param,iterations,elapsed_sec,throughput" > combined/all-performance-data.csv

        # Append all data (skip headers from individual files)        

        find artifacts -name 'performance-results.csv' -exec tail -n +2 {} \; >> combined/all-performance-data.csv        # Append all data (skip headers from individual files)

                find artifacts -name 'performance-results.csv' -exec tail -n +2 {} \; >> combined/all-performance-data.csv

        echo "‚úÖ Merged performance data:"        

        wc -l combined/all-performance-data.csv        echo "‚úÖ Merged performance data:"

        head -20 combined/all-performance-data.csv        wc -l combined/all-performance-data.csv

            head -20 combined/all-performance-data.csv

    - name: Upload combined performance data    

      uses: actions/upload-artifact@v4    - name: Upload combined performance data

      with:      uses: actions/upload-artifact@v4

        name: combined-performance-data      with:

        path: combined/all-performance-data.csv        name: combined-performance-data

        retention-days: 90        path: combined/all-performance-data.csv

          retention-days: 90

  # Analyze performance data and generate reports  

  analyze-performance:  # Job 2: Analyze performance data and generate reports

    name: Analyze Performance Results  analyze-performance:

    runs-on: ubuntu-latest    name: Analyze Performance Results

    needs: collect-performance-data    runs-on: ubuntu-latest

        needs: collect-performance-data

    steps:    

    - uses: actions/checkout@v4    steps:

        - uses: actions/checkout@v4

    - name: Install uv    

      uses: astral-sh/setup-uv@v5    - name: Install uv

      with:      uses: astral-sh/setup-uv@v5

        enable-cache: true      with:

            enable-cache: true

    - name: Install Python dependencies    

      run: |    - name: Install Python dependencies

        cd scripts      run: |

        uv sync        cd scripts

            uv sync

    - name: Download combined performance data    

      uses: actions/download-artifact@v4    - name: Download combined performance data

      with:      uses: actions/download-artifact@v4

        name: combined-performance-data      with:

        path: data/        name: combined-performance-data

            path: data/

    - name: Run performance analysis    

      run: |    - name: Run performance analysis

        mkdir -p docs/performance      run: |

        cd scripts        mkdir -p docs

        uv run python analyze_performance.py ../data/all-performance-data.csv --output-dir ../docs/performance        cd scripts

            uv run python analyze_performance.py ../data/all-performance-data.csv --output-dir ../docs

    - name: Generate index page    

      run: |    - name: Generate index page

        cat > docs/performance/index.html << 'EOF'      run: |

        <!DOCTYPE html>        cat > docs/index.html << 'EOF'

        <html lang="en">        <!DOCTYPE html>

        <head>        <html lang="en">

            <meta charset="UTF-8">        <head>

            <meta name="viewport" content="width=device-width, initial-scale=1.0">            <meta charset="UTF-8">

            <title>fastcond Performance Results</title>            <meta name="viewport" content="width=device-width, initial-scale=1.0">

            <style>            <title>fastcond Performance Results</title>

                body {            <style>

                    font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, sans-serif;                body {

                    max-width: 1200px;                    font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, sans-serif;

                    margin: 0 auto;                    max-width: 1200px;

                    padding: 20px;                    margin: 0 auto;

                    line-height: 1.6;                    padding: 20px;

                }                    line-height: 1.6;

                h1 { color: #2c3e50; border-bottom: 3px solid #3498db; padding-bottom: 10px; }                }

                h2 { color: #34495e; margin-top: 30px; }                h1 { color: #2c3e50; border-bottom: 3px solid #3498db; padding-bottom: 10px; }

                img { max-width: 100%; height: auto; margin: 20px 0; box-shadow: 0 2px 8px rgba(0,0,0,0.1); }                h2 { color: #34495e; margin-top: 30px; }

                .chart-container { margin: 30px 0; }                img { max-width: 100%; height: auto; margin: 20px 0; box-shadow: 0 2px 8px rgba(0,0,0,0.1); }

                table { border-collapse: collapse; width: 100%; margin: 20px 0; }                .chart-container { margin: 30px 0; }

                th, td { border: 1px solid #ddd; padding: 12px; text-align: left; }                table { border-collapse: collapse; width: 100%; margin: 20px 0; }

                th { background-color: #3498db; color: white; }                th, td { border: 1px solid #ddd; padding: 12px; text-align: left; }

                tr:nth-child(even) { background-color: #f2f2f2; }                th { background-color: #3498db; color: white; }

                .footer { margin-top: 50px; padding-top: 20px; border-top: 1px solid #ddd; color: #7f8c8d; }                tr:nth-child(even) { background-color: #f2f2f2; }

            </style>                .footer { margin-top: 50px; padding-top: 20px; border-top: 1px solid #ddd; color: #7f8c8d; }

        </head>            </style>

        <body>        </head>

            <h1>üöÄ fastcond Performance Benchmarks</h1>        <body>

            <p>Cross-platform performance comparison of fastcond synchronization primitives vs native implementations.</p>            <h1>üöÄ fastcond Performance Benchmarks</h1>

                        <p>Cross-platform performance comparison of fastcond synchronization primitives vs native implementations.</p>

            <h2>üìä Performance Comparison Charts</h2>            

            <div class="chart-container">            <h2>üìä Performance Comparison Charts</h2>

                <h3>Throughput Comparison</h3>            <div class="chart-container">

                <img src="performance-comparison.png" alt="Performance Comparison Across Platforms">                <h3>Throughput Comparison</h3>

            </div>                <img src="performance-comparison.png" alt="Performance Comparison Across Platforms">

                        </div>

            <div class="chart-container">            

                <h3>Speedup vs Native</h3>            <div class="chart-container">

                <img src="speedup-comparison.png" alt="Speedup Comparison">                <h3>Speedup vs Native</h3>

            </div>                <img src="speedup-comparison.png" alt="Speedup Comparison">

                        </div>

            <h2>üìà Detailed Results</h2>            

            <p>See <a href="performance-comparison.md">performance-comparison.md</a> for detailed benchmark results.</p>            <h2>üìà Detailed Results</h2>

            <p>Download raw data: <a href="performance-summary.json">performance-summary.json</a></p>            <p>See <a href="performance-comparison.md">performance-comparison.md</a> for detailed benchmark results.</p>

                        <p>Download raw data: <a href="performance-summary.json">performance-summary.json</a></p>

            <div class="footer">            

                <p>Generated: ${date -u +"%Y-%m-%d %H:%M:%S UTC"}</p>            <div class="footer">

                <p>Repository: <a href="https://github.com/kristjanvalur/fastcond">kristjanvalur/fastcond</a></p>                <p>Generated: $(date -u +"%Y-%m-%d %H:%M:%S UTC")</p>

            </div>                <p>Repository: <a href="https://github.com/kristjanvalur/fastcond">kristjanvalur/fastcond</a></p>

        </body>            </div>

        </html>        </body>

        EOF        </html>

            EOF

    - name: Upload analysis results    

      uses: actions/upload-artifact@v4    - name: Upload analysis results

      with:      uses: actions/upload-artifact@v4

        name: performance-analysis      with:

        path: |        name: performance-analysis

          docs/performance/performance-comparison.md        path: |

          docs/performance/performance-summary.json          docs/performance-comparison.md

          docs/performance/performance-comparison.png          docs/performance-summary.json

          docs/performance/speedup-comparison.png          docs/performance-comparison.png

          docs/performance/index.html          docs/speedup-comparison.png

        retention-days: 90          docs/index.html

            retention-days: 90

    - name: Deploy to GitHub Pages    

      if: github.ref == 'refs/heads/master' && github.event_name == 'push'    - name: Deploy to GitHub Pages

      uses: peaceiris/actions-gh-pages@v4      if: github.ref == 'refs/heads/master' && github.event_name == 'push'

      with:      uses: peaceiris/actions-gh-pages@v4

        github_token: ${{ secrets.GITHUB_TOKEN }}      with:

        publish_dir: ./docs/performance        github_token: ${{ secrets.GITHUB_TOKEN }}

        keep_files: true        publish_dir: ./docs

        destination_dir: performance        keep_files: true

        destination_dir: performance
    
    - name: Install uv
      uses: astral-sh/setup-uv@v5
      with:
        enable-cache: true
    
    - name: Install build dependencies
      run: |
        sudo apt-get update
        sudo apt-get install -y build-essential cmake
    
    - name: Install Python dependencies
      run: |
        cd scripts
        uv sync
    
    - name: Build tests (Release mode for accurate benchmarks)
      run: |
        cd test
        make clean
        make all CFLAGS="-O3 -DNDEBUG"
    
    - name: Run benchmark suite
      run: |
        echo "üîç Before benchmark generation - docs/ contents:"
        ls -la docs/ || echo "docs/ directory not found"
        echo "üöÄ Running benchmark suite..."
        BUILD_DIR=test ./scripts/run_benchmarks.sh
        echo "‚úÖ After benchmark generation - docs/ contents:"
        ls -la docs/
        echo "üìä Generated files details:"
        find docs/ -name "*.json" -o -name "*.html" -o -name "*.png" | xargs ls -la 2>/dev/null || echo "No generated files found"
    
    - name: Run GIL fairness analysis
      run: |
        echo "üîç Before GIL analysis - docs/ contents:"
        ls -la docs/
        # Ensure GIL test executables are available
        cd test && ls -la gil_test_* || echo "No GIL tests found"
        cd ..
        echo "üé≠ Running GIL fairness analysis..."
        BUILD_DIR=test ./scripts/run_gil_fairness_analysis.sh
        echo "‚úÖ After GIL analysis - docs/ contents:"
        ls -la docs/
        echo "üìä All generated files summary:"
        find docs/ -name "*.json" -o -name "*.html" -o -name "*.png" | xargs ls -la 2>/dev/null || echo "No generated files found"
        
    - name: Final diagnostics before deployment
      run: |
        echo "üéØ FINAL GENERATION SUMMARY"
        echo "========================================="
        echo "üìÅ Working directory: $(pwd)"
        echo "üìä Complete docs/ directory listing:"
        ls -la docs/
        echo ""
        echo "üìà Expected files check:"
        expected_files=("index.html" "gil-fairness.html" "benchmark-results.json" "benchmark-results.md" "gil-fairness-results.json" "throughput-comparison.png" "latency-comparison.png" "gil_fairness_comparison.png")
        for file in "${expected_files[@]}"; do
          if [ -f "docs/$file" ]; then
            size=$(stat -c%s "docs/$file" 2>/dev/null || echo "unknown")
            echo "  ‚úÖ $file ($size bytes)"
          else
            echo "  ‚ùå $file (MISSING)"
          fi
        done
        echo ""
        echo "üìÇ Total file count: $(find docs/ -type f | wc -l)"
        echo "üíæ Total size: $(du -sh docs/ | cut -f1)"
        echo "========================================="
    
    - name: Upload benchmark results
      run: |
        echo "üì¶ Preparing artifacts for upload..."
        echo "üìä Files to be uploaded:"
        for file in docs/benchmark-results.json docs/benchmark-results.md docs/index.html docs/throughput-comparison.png docs/latency-comparison.png docs/gil-fairness-results.json docs/gil-fairness.html docs/gil_fairness_comparison.png; do
          if [ -f "$file" ]; then
            echo "  ‚úÖ $file ($(ls -lh "$file" | awk '{print $5}'))"
          else
            echo "  ‚ùå $file (missing)"
          fi
        done
        
    - name: Store benchmark artifacts
      uses: actions/upload-artifact@v4
      with:
        name: benchmark-results-${{ github.sha }}
        path: |
          docs/benchmark-results.json
          docs/benchmark-results.md
          docs/index.html
          docs/throughput-comparison.png
          docs/latency-comparison.png
          docs/gil-fairness-results.json
          docs/gil-fairness.html
          docs/gil_fairness_comparison.png
        retention-days: 90
    
    - name: Comment PR with results
      if: github.event_name == 'pull_request'
      uses: actions/github-script@v7
      with:
        script: |
          const fs = require('fs');
          const resultsTable = fs.readFileSync('docs/benchmark-results.md', 'utf8');
          
          // Create comment body
          const body = `## üìä Performance Benchmark Results
          
          ${resultsTable}
          
          <details>
          <summary>View Charts</summary>
          
          See the workflow artifacts for throughput and latency comparison charts.
          
          </details>
          
          ---
          *Benchmarked on: \`${{ runner.os }}\` with \`${{ runner.arch }}\`*
          *Commit: \`${{ github.sha }}\`*`;
          
          github.rest.issues.createComment({
            issue_number: context.issue.number,
            owner: context.repo.owner,
            repo: context.repo.repo,
            body: body
          });
    
    - name: Prepare files for GitHub Pages deployment
      if: |
        (github.ref == 'refs/heads/master' && github.event_name == 'push') ||
        (github.event_name == 'pull_request_target' && github.event.action == 'closed' && github.event.pull_request.merged == true)
      run: |
        echo "üöÄ Preparing GitHub Pages deployment..."
        echo "üìÅ Current working directory: $(pwd)"
        echo "üìä docs/ directory contents before deployment:"
        ls -la docs/
        echo "üîç Generated files verification:"
        echo "  index.html exists: $(test -f docs/index.html && echo 'YES' || echo 'NO')"
        echo "  gil-fairness.html exists: $(test -f docs/gil-fairness.html && echo 'YES' || echo 'NO')"
        echo "  benchmark-results.json exists: $(test -f docs/benchmark-results.json && echo 'YES' || echo 'NO')"
        echo "üìã Detailed file listing with sizes:"
        find docs/ -type f -exec ls -lh {} \; 2>/dev/null || echo "No files found in docs/"
        
        echo ""
        echo "üîß Creating deployment staging area (ignoring .gitignore)..."
        # Create a temporary staging area for deployment
        mkdir -p docs-deploy
        
        # Copy all files from docs/ to docs-deploy/, including gitignored files
        cp -r docs/* docs-deploy/ 2>/dev/null || true
        
        echo "üìÅ Deployment staging area contents:"
        ls -la docs-deploy/
        echo "üìè File count in deployment area: $(find docs-deploy/ -type f | wc -l)"
        echo "üíæ Total deployment size: $(du -sh docs-deploy/ | cut -f1)"
      
    - name: Deploy to GitHub Pages
      if: |
        (github.ref == 'refs/heads/master' && github.event_name == 'push') ||
        (github.event_name == 'pull_request_target' && github.event.action == 'closed' && github.event.pull_request.merged == true)
      uses: peaceiris/actions-gh-pages@v4
      with:
        github_token: ${{ secrets.GITHUB_TOKEN }}
        publish_dir: ./docs-deploy       # Use staging area that bypasses .gitignore
        keep_files: false               # Always replace entire content
        allow_empty_commit: true        # Force deployment even if content appears unchanged
        force_orphan: true              # Force orphan gh-pages branch (clean history)
        publish_branch: gh-pages        # Target branch
        enable_jekyll: false            # Disable Jekyll processing
        exclude_assets: '.github'       # Don't include .github folder in deployment

  compare-baseline:
    name: Compare with Baseline
    runs-on: ubuntu-latest
    if: github.event_name == 'pull_request'
    
    steps:
    - uses: actions/checkout@v4
      with:
        fetch-depth: 0
    
    - name: Install uv
      uses: astral-sh/setup-uv@v5
    
    - name: Install dependencies
      run: |
        sudo apt-get update
        sudo apt-get install -y build-essential cmake
        cd scripts
        uv sync
    
    # Run benchmarks on PR branch
    - name: Build and benchmark PR
      run: |
        cd test && make clean && make all CFLAGS="-O3 -DNDEBUG"
        cd ..
        BUILD_DIR=test ./scripts/run_benchmarks.sh
        mv docs/benchmark-results.json /tmp/pr-results.json
    
    # Checkout and run benchmarks on base branch
    - name: Checkout base branch
      run: |
        # Clean any generated files (not tracked in git)
        rm -rf docs/benchmark-results.* docs/index.html docs/*-comparison.png || true
        git fetch origin ${{ github.base_ref }}
        git checkout origin/${{ github.base_ref }}
    
    - name: Build and benchmark base
      id: base_benchmark
      continue-on-error: true
      run: |
        echo "Attempting to build and benchmark base branch..."
        if cd test && make clean && make all CFLAGS="-O3 -DNDEBUG" 2>/dev/null; then
          cd ..
          if BUILD_DIR=test ./scripts/run_benchmarks.sh 2>/dev/null; then
            mv docs/benchmark-results.json /tmp/base-results.json
            echo "base_success=true" >> $GITHUB_OUTPUT
          else
            echo "‚ö†Ô∏è Base branch benchmark execution failed - benchmark infrastructure may be incompatible"
            echo "base_success=false" >> $GITHUB_OUTPUT
          fi
        else
          echo "‚ö†Ô∏è Base branch build failed - benchmark infrastructure may be missing"
          echo "base_success=false" >> $GITHUB_OUTPUT
        fi
    
    - name: Compare results
      run: |
        # Create a robust comparison script that handles missing/incompatible base data
        cat > compare.py << 'PYEOF'
        import json
        import sys
        import os
        
        def safe_load_results(path):
            try:
                with open(path) as f:
                    data = json.load(f)
                    # Validate structure
                    if isinstance(data, list) and len(data) > 0:
                        return {r['benchmark'] + '_' + r['implementation']: r for r in data}
                    return {}
            except (FileNotFoundError, json.JSONDecodeError, KeyError) as e:
                print(f"‚ö†Ô∏è Could not load {path}: {e}")
                return {}
        
        def main():
            # Load results with error handling
            base = safe_load_results('/tmp/base-results.json')
            pr = safe_load_results('/tmp/pr-results.json')
            
            if not base:
                print("## üìä Performance Benchmarks (Base Branch Unavailable)\n")
                print("‚ö†Ô∏è **Base branch comparison unavailable** - benchmark infrastructure may be incompatible\n")
                print("### PR Branch Results Summary\n")
                
                if pr:
                    for key in sorted(pr.keys()):
                        pr_throughput = pr[key]['results']['overall'].get('throughput_items_per_sec', 0)
                        print(f"‚úÖ **{key}**: {pr_throughput:,.0f} items/sec")
                else:
                    print("‚ùå No valid benchmark results available")
                return
            
            print("## üî¨ Performance Comparison vs Base Branch\n")
            
            compared_any = False
            for key in sorted(pr.keys()):
                if key not in base:
                    print(f"üÜï **{key}**: New benchmark (no baseline)")
                    continue
                
                pr_throughput = pr[key]['results']['overall'].get('throughput_items_per_sec', 0)
                base_throughput = base[key]['results']['overall'].get('throughput_items_per_sec', 0)
                
                if base_throughput > 0:
                    change = ((pr_throughput / base_throughput) - 1) * 100
                    emoji = "üü¢" if change > 2 else "üî¥" if change < -2 else "‚ö™"
                    print(f"{emoji} **{key}**: {change:+.1f}% ({pr_throughput:,.0f} vs {base_throughput:,.0f} items/sec)")
                    compared_any = True
            
            if not compared_any:
                print("‚ö†Ô∏è No benchmarks could be compared - likely incompatible result formats")
        
        if __name__ == "__main__":
            main()
        PYEOF
        
        cd scripts
        uv run python ../compare.py > /tmp/comparison.md
        cat /tmp/comparison.md
    
    - name: Comment comparison
      uses: actions/github-script@v7
      with:
        script: |
          const fs = require('fs');
          const comparison = fs.readFileSync('/tmp/comparison.md', 'utf8');
          
          github.rest.issues.createComment({
            issue_number: context.issue.number,
            owner: context.repo.owner,
            repo: context.repo.repo,
            body: `## üî¨ Performance Regression Check\n\n${comparison}\n\n---\n*Changes > ¬±2% are highlighted*`
          });
