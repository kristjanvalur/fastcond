name: Performance Benchmarks

on:
  pull_request:
    branches: [ master, main, develop ]
  workflow_dispatch:  # Allow manual trigger
  schedule:
    # Run weekly on Monday at 00:00 UTC to track performance over time
    - cron: '0 0 * * 1'

jobs:
  # Original benchmark job for scheduled runs and manual triggers
  benchmark:
    name: Run Performance Benchmarks
    runs-on: ubuntu-latest
    timeout-minutes: 15
    
    steps:
    - uses: actions/checkout@v4
      with:
        fetch-depth: 0  # Fetch all history for performance tracking
    
    - name: Install uv
      uses: astral-sh/setup-uv@v5
      with:
        enable-cache: true
    
    - name: Install build dependencies
      run: |
        sudo apt-get update
        sudo apt-get install -y build-essential cmake
    
    - name: Install Python dependencies
      run: |
        cd scripts
        uv sync
    
    - name: Build tests (Release mode for accurate benchmarks)
      run: |
        cd test
        make clean
        make all CFLAGS="-O3 -DNDEBUG"
    
    - name: Run benchmark suite
      run: |
        echo "🔍 Before benchmark generation - docs/ contents:"
        ls -la docs/ || echo "docs/ directory not found"
        echo "🚀 Running benchmark suite..."
        BUILD_DIR=test ./scripts/run_benchmarks.sh
        echo "✅ After benchmark generation - docs/ contents:"
        ls -la docs/
        echo "📊 Generated files details:"
        find docs/ -name "*.json" -o -name "*.html" -o -name "*.png" | xargs ls -la 2>/dev/null || echo "No generated files found"
    
    - name: Run GIL fairness analysis
      run: |
        echo "🔍 Before GIL analysis - docs/ contents:"
        ls -la docs/
        # Ensure GIL test executables are available
        cd test && ls -la gil_test_* || echo "No GIL tests found"
        cd ..
        echo "🎭 Running GIL fairness analysis..."
        BUILD_DIR=test ./scripts/run_gil_fairness_analysis.sh
        echo "✅ After GIL analysis - docs/ contents:"
        ls -la docs/
        echo "📊 All generated files summary:"
        find docs/ -name "*.json" -o -name "*.html" -o -name "*.png" | xargs ls -la 2>/dev/null || echo "No generated files found"
        
    - name: Final diagnostics before deployment
      run: |
        echo "🎯 FINAL GENERATION SUMMARY"
        echo "========================================="
        echo "📁 Working directory: $(pwd)"
        echo "📊 Complete docs/ directory listing:"
        ls -la docs/
        echo ""
        echo "📈 Expected files check:"
        expected_files=("index.html" "gil-fairness.html" "benchmark-results.json" "benchmark-results.md" "gil-fairness-results.json" "throughput-comparison.png" "latency-comparison.png" "gil_fairness_comparison.png")
        for file in "${expected_files[@]}"; do
          if [ -f "docs/$file" ]; then
            size=$(stat -c%s "docs/$file" 2>/dev/null || echo "unknown")
            echo "  ✅ $file ($size bytes)"
          else
            echo "  ❌ $file (MISSING)"
          fi
        done
        echo ""
        echo "📂 Total file count: $(find docs/ -type f | wc -l)"
        echo "💾 Total size: $(du -sh docs/ | cut -f1)"
        echo "========================================="
    
    - name: Upload benchmark results
      run: |
        echo "📦 Preparing artifacts for upload..."
        echo "📊 Files to be uploaded:"
        for file in docs/benchmark-results.json docs/benchmark-results.md docs/index.html docs/throughput-comparison.png docs/latency-comparison.png docs/gil-fairness-results.json docs/gil-fairness.html docs/gil_fairness_comparison.png; do
          if [ -f "$file" ]; then
            echo "  ✅ $file ($(ls -lh "$file" | awk '{print $5}'))"
          else
            echo "  ❌ $file (missing)"
          fi
        done
        
    - name: Store benchmark artifacts
      uses: actions/upload-artifact@v4
      with:
        name: benchmark-results-${{ github.sha }}
        path: |
          docs/benchmark-results.json
          docs/benchmark-results.md
          docs/index.html
          docs/throughput-comparison.png
          docs/latency-comparison.png
          docs/gil-fairness-results.json
          docs/gil-fairness.html
          docs/gil_fairness_comparison.png
        retention-days: 90
    
    - name: Comment PR with results
      if: github.event_name == 'pull_request'
      uses: actions/github-script@v7
      with:
        script: |
          const fs = require('fs');
          const resultsTable = fs.readFileSync('docs/benchmark-results.md', 'utf8');
          
          // Create comment body
          const body = `## 📊 Performance Benchmark Results
          
          ${resultsTable}
          
          <details>
          <summary>View Charts</summary>
          
          See the workflow artifacts for throughput and latency comparison charts.
          
          </details>
          
          ---
          *Benchmarked on: \`${{ runner.os }}\` with \`${{ runner.arch }}\`*
          *Commit: \`${{ github.sha }}\`*`;
          
          github.rest.issues.createComment({
            issue_number: context.issue.number,
            owner: context.repo.owner,
            repo: context.repo.repo,
            body: body
          });

  compare-baseline:
    name: Compare with Baseline
    runs-on: ubuntu-latest
    timeout-minutes: 15
    if: github.event_name == 'pull_request'
    
    steps:
    - uses: actions/checkout@v4
      with:
        fetch-depth: 0
    
    - name: Install uv
      uses: astral-sh/setup-uv@v5
    
    - name: Install dependencies
      run: |
        sudo apt-get update
        sudo apt-get install -y build-essential cmake
        cd scripts
        uv sync
    
    # Run benchmarks on PR branch
    - name: Build and benchmark PR
      run: |
        cd test && make clean && make all CFLAGS="-O3 -DNDEBUG"
        cd ..
        BUILD_DIR=test ./scripts/run_benchmarks.sh
        mv docs/benchmark-results.json /tmp/pr-results.json
    
    # Checkout and run benchmarks on base branch
    - name: Checkout base branch
      run: |
        # Clean any generated files (not tracked in git)
        rm -rf docs/benchmark-results.* docs/index.html docs/*-comparison.png || true
        git fetch origin ${{ github.base_ref }}
        git checkout origin/${{ github.base_ref }}
    
    - name: Build and benchmark base
      id: base_benchmark
      continue-on-error: true
      run: |
        echo "Attempting to build and benchmark base branch..."
        if cd test && make clean && make all CFLAGS="-O3 -DNDEBUG" 2>/dev/null; then
          cd ..
          if BUILD_DIR=test ./scripts/run_benchmarks.sh 2>/dev/null; then
            mv docs/benchmark-results.json /tmp/base-results.json
            echo "base_success=true" >> $GITHUB_OUTPUT
          else
            echo "⚠️ Base branch benchmark execution failed - benchmark infrastructure may be incompatible"
            echo "base_success=false" >> $GITHUB_OUTPUT
          fi
        else
          echo "⚠️ Base branch build failed - benchmark infrastructure may be missing"
          echo "base_success=false" >> $GITHUB_OUTPUT
        fi
    
    - name: Compare results
      run: |
        # Create a robust comparison script that handles missing/incompatible base data
        cat > compare.py << 'PYEOF'
        import json
        import sys
        import os
        
        def safe_load_results(path):
            try:
                with open(path) as f:
                    data = json.load(f)
                    # Validate structure
                    if isinstance(data, list) and len(data) > 0:
                        return {r['benchmark'] + '_' + r['implementation']: r for r in data}
                    return {}
            except (FileNotFoundError, json.JSONDecodeError, KeyError) as e:
                print(f"⚠️ Could not load {path}: {e}")
                return {}
        
        def main():
            # Load results with error handling
            base = safe_load_results('/tmp/base-results.json')
            pr = safe_load_results('/tmp/pr-results.json')
            
            if not base:
                print("## 📊 Performance Benchmarks (Base Branch Unavailable)\n")
                print("⚠️ **Base branch comparison unavailable** - benchmark infrastructure may be incompatible\n")
                print("### PR Branch Results Summary\n")
                
                if pr:
                    for key in sorted(pr.keys()):
                        pr_throughput = pr[key]['results']['overall'].get('throughput_items_per_sec', 0)
                        print(f"✅ **{key}**: {pr_throughput:,.0f} items/sec")
                else:
                    print("❌ No valid benchmark results available")
                return
            
            print("## 🔬 Performance Comparison vs Base Branch\n")
            
            compared_any = False
            for key in sorted(pr.keys()):
                if key not in base:
                    print(f"🆕 **{key}**: New benchmark (no baseline)")
                    continue
                
                pr_throughput = pr[key]['results']['overall'].get('throughput_items_per_sec', 0)
                base_throughput = base[key]['results']['overall'].get('throughput_items_per_sec', 0)
                
                if base_throughput > 0:
                    change = ((pr_throughput / base_throughput) - 1) * 100
                    emoji = "🟢" if change > 2 else "🔴" if change < -2 else "⚪"
                    print(f"{emoji} **{key}**: {change:+.1f}% ({pr_throughput:,.0f} vs {base_throughput:,.0f} items/sec)")
                    compared_any = True
            
            if not compared_any:
                print("⚠️ No benchmarks could be compared - likely incompatible result formats")
        
        if __name__ == "__main__":
            main()
        PYEOF
        
        cd scripts
        uv run python ../compare.py > /tmp/comparison.md
        cat /tmp/comparison.md
    
    - name: Comment comparison
      uses: actions/github-script@v7
      with:
        script: |
          const fs = require('fs');
          const comparison = fs.readFileSync('/tmp/comparison.md', 'utf8');
          
          github.rest.issues.createComment({
            issue_number: context.issue.number,
            owner: context.repo.owner,
            repo: context.repo.repo,
            body: `## 🔬 Performance Regression Check\n\n${comparison}\n\n---\n*Changes > ±2% are highlighted*`
          });
