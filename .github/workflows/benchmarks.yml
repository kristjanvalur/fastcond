name: Performance Benchmarks

on:
  # Trigger after CI workflow completes (will have artifacts)
  workflow_run:
    workflows: ["CI"]
    types:
      - completed
  push:
    branches: [ master, main, perf-figures-improvements ]
  pull_request:
    branches: [ master, main ]
  workflow_dispatch:  # Allow manual trigger
  schedule:
    # Run weekly on Monday at 00:00 UTC to track performance over time
    - cron: '0 0 * * 1'

jobs:
  # Job 1: Collect performance data from CI artifacts or run locally
  collect-performance-data:
    name: Collect Performance Data
    runs-on: ubuntu-latest
    
    steps:
    - uses: actions/checkout@v4
      with:
        fetch-depth: 0
    
    # Try to download artifacts from CI workflow
    - name: Download CI performance artifacts
      if: github.event_name == 'workflow_run'
      uses: actions/download-artifact@v4
      with:
        pattern: perf-*
        path: artifacts/
        merge-multiple: false
        run-id: ${{ github.event.workflow_run.id }}
      continue-on-error: true
    
    # If no artifacts available, run benchmarks locally
    - name: Check if artifacts were downloaded
      id: check_artifacts
      run: |
        if [ -d "artifacts" ] && [ "$(find artifacts -name '*.csv' | wc -l)" -gt 0 ]; then
          echo "has_artifacts=true" >> $GITHUB_OUTPUT
          echo "✅ Found CI artifacts, will use them"
        else
          echo "has_artifacts=false" >> $GITHUB_OUTPUT
          echo "⚠️  No CI artifacts found, will run benchmarks locally"
        fi
    
    - name: Install build dependencies (if running locally)
      if: steps.check_artifacts.outputs.has_artifacts == 'false'
      run: |
        sudo apt-get update
        sudo apt-get install -y build-essential cmake
    
    - name: Build and run benchmarks locally (fallback)
      if: steps.check_artifacts.outputs.has_artifacts == 'false'
      run: |
        # Build tests
        cmake -B build -DCMAKE_BUILD_TYPE=Release -DFASTCOND_BUILD_TESTS=ON
        cmake --build build -j$(nproc)
        
        # Create artifacts directory structure
        mkdir -p artifacts/perf-Linux-gcc
        
        # Run benchmarks with local build
        ./scripts/run_performance_benchmarks.sh build artifacts/perf-Linux-gcc/performance-results.csv
        
        echo "✅ Local benchmarks complete"
    
    - name: Merge all performance data
      run: |
        # Combine all CSV files into one
        mkdir -p combined
        
        # Write header
        echo "platform,os_version,test,variant,threads,param,iterations,elapsed_sec,throughput" > combined/all-performance-data.csv
        
        # Append all data (skip headers from individual files)
        find artifacts -name 'performance-results.csv' -exec tail -n +2 {} \; >> combined/all-performance-data.csv
        
        echo "✅ Merged performance data:"
        wc -l combined/all-performance-data.csv
        head -20 combined/all-performance-data.csv
    
    - name: Upload combined performance data
      uses: actions/upload-artifact@v4
      with:
        name: combined-performance-data
        path: combined/all-performance-data.csv
        retention-days: 90

  # Job 2: Analyze performance data and generate reports
  analyze-performance:
    name: Analyze Performance Results
    runs-on: ubuntu-latest
    needs: collect-performance-data
    
    steps:
    - uses: actions/checkout@v4
    
    - name: Install uv
      uses: astral-sh/setup-uv@v5
      with:
        enable-cache: true
    
    - name: Install Python dependencies
      run: |
        cd scripts
        uv sync
    
    - name: Download combined performance data
      uses: actions/download-artifact@v4
      with:
        name: combined-performance-data
        path: data/
    
    - name: Run performance analysis
      run: |
        mkdir -p docs
        cd scripts
        uv run python analyze_performance.py ../data/all-performance-data.csv --output-dir ../docs
    
    - name: Generate index page
      run: |
        cat > docs/index.html << 'EOF'
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>fastcond Performance Results</title>
    <style>
        body {
            font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, sans-serif;
            max-width: 1200px;
            margin: 0 auto;
            padding: 20px;
            line-height: 1.6;
        }
        h1 { color: #2c3e50; border-bottom: 3px solid #3498db; padding-bottom: 10px; }
        h2 { color: #34495e; margin-top: 30px; }
        img { max-width: 100%; height: auto; margin: 20px 0; box-shadow: 0 2px 8px rgba(0,0,0,0.1); }
        .chart-container { margin: 30px 0; }
        table { border-collapse: collapse; width: 100%; margin: 20px 0; }
        th, td { border: 1px solid #ddd; padding: 12px; text-align: left; }
        th { background-color: #3498db; color: white; }
        tr:nth-child(even) { background-color: #f2f2f2; }
        .footer { margin-top: 50px; padding-top: 20px; border-top: 1px solid #ddd; color: #7f8c8d; }
    </style>
</head>
<body>
    <h1>🚀 fastcond Performance Benchmarks</h1>
    <p>Cross-platform performance comparison of fastcond synchronization primitives vs native implementations.</p>
    
    <h2>📊 Performance Comparison Charts</h2>
    <div class="chart-container">
        <h3>Throughput Comparison</h3>
        <img src="performance-comparison.png" alt="Performance Comparison Across Platforms">
    </div>
    
    <div class="chart-container">
        <h3>Speedup vs Native</h3>
        <img src="speedup-comparison.png" alt="Speedup Comparison">
    </div>
    
    <h2>📈 Detailed Results</h2>
    <p>See <a href="performance-comparison.md">performance-comparison.md</a> for detailed benchmark results.</p>
    <p>Download raw data: <a href="performance-summary.json">performance-summary.json</a></p>
    
    <div class="footer">
        <p>Generated: $(date -u +"%Y-%m-%d %H:%M:%S UTC")</p>
        <p>Repository: <a href="https://github.com/kristjanvalur/fastcond">kristjanvalur/fastcond</a></p>
    </div>
</body>
</html>
EOF
    
    - name: Upload analysis results
      uses: actions/upload-artifact@v4
      with:
        name: performance-analysis
        path: |
          docs/performance-comparison.md
          docs/performance-summary.json
          docs/performance-comparison.png
          docs/speedup-comparison.png
          docs/index.html
        retention-days: 90
    
    - name: Deploy to GitHub Pages
      if: github.ref == 'refs/heads/master' && github.event_name == 'push'
      uses: peaceiris/actions-gh-pages@v4
      with:
        github_token: ${{ secrets.GITHUB_TOKEN }}
        publish_dir: ./docs
        keep_files: true
        destination_dir: performance

  # Job 3: Original benchmark job (kept for backwards compatibility)
  benchmark:
    name: Run Performance Benchmarks
    runs-on: ubuntu-latest
    
    steps:
    - uses: actions/checkout@v4
      with:
        fetch-depth: 0  # Fetch all history for performance tracking
    
    - name: Install uv
      uses: astral-sh/setup-uv@v5
      with:
        enable-cache: true
    
    - name: Install build dependencies
      run: |
        sudo apt-get update
        sudo apt-get install -y build-essential cmake
    
    - name: Install Python dependencies
      run: |
        cd scripts
        uv sync
    
    - name: Build tests (Release mode for accurate benchmarks)
      run: |
        cd test
        make clean
        make all CFLAGS="-O3 -DNDEBUG"
    
    - name: Run benchmark suite
      run: |
        echo "🔍 Before benchmark generation - docs/ contents:"
        ls -la docs/ || echo "docs/ directory not found"
        echo "🚀 Running benchmark suite..."
        BUILD_DIR=test ./scripts/run_benchmarks.sh
        echo "✅ After benchmark generation - docs/ contents:"
        ls -la docs/
        echo "📊 Generated files details:"
        find docs/ -name "*.json" -o -name "*.html" -o -name "*.png" | xargs ls -la 2>/dev/null || echo "No generated files found"
    
    - name: Run GIL fairness analysis
      run: |
        echo "🔍 Before GIL analysis - docs/ contents:"
        ls -la docs/
        # Ensure GIL test executables are available
        cd test && ls -la gil_test_* || echo "No GIL tests found"
        cd ..
        echo "🎭 Running GIL fairness analysis..."
        BUILD_DIR=test ./scripts/run_gil_fairness_analysis.sh
        echo "✅ After GIL analysis - docs/ contents:"
        ls -la docs/
        echo "📊 All generated files summary:"
        find docs/ -name "*.json" -o -name "*.html" -o -name "*.png" | xargs ls -la 2>/dev/null || echo "No generated files found"
        
    - name: Final diagnostics before deployment
      run: |
        echo "🎯 FINAL GENERATION SUMMARY"
        echo "========================================="
        echo "📁 Working directory: $(pwd)"
        echo "📊 Complete docs/ directory listing:"
        ls -la docs/
        echo ""
        echo "📈 Expected files check:"
        expected_files=("index.html" "gil-fairness.html" "benchmark-results.json" "benchmark-results.md" "gil-fairness-results.json" "throughput-comparison.png" "latency-comparison.png" "gil_fairness_comparison.png")
        for file in "${expected_files[@]}"; do
          if [ -f "docs/$file" ]; then
            size=$(stat -c%s "docs/$file" 2>/dev/null || echo "unknown")
            echo "  ✅ $file ($size bytes)"
          else
            echo "  ❌ $file (MISSING)"
          fi
        done
        echo ""
        echo "📂 Total file count: $(find docs/ -type f | wc -l)"
        echo "💾 Total size: $(du -sh docs/ | cut -f1)"
        echo "========================================="
    
    - name: Upload benchmark results
      run: |
        echo "📦 Preparing artifacts for upload..."
        echo "📊 Files to be uploaded:"
        for file in docs/benchmark-results.json docs/benchmark-results.md docs/index.html docs/throughput-comparison.png docs/latency-comparison.png docs/gil-fairness-results.json docs/gil-fairness.html docs/gil_fairness_comparison.png; do
          if [ -f "$file" ]; then
            echo "  ✅ $file ($(ls -lh "$file" | awk '{print $5}'))"
          else
            echo "  ❌ $file (missing)"
          fi
        done
        
    - name: Store benchmark artifacts
      uses: actions/upload-artifact@v4
      with:
        name: benchmark-results-${{ github.sha }}
        path: |
          docs/benchmark-results.json
          docs/benchmark-results.md
          docs/index.html
          docs/throughput-comparison.png
          docs/latency-comparison.png
          docs/gil-fairness-results.json
          docs/gil-fairness.html
          docs/gil_fairness_comparison.png
        retention-days: 90
    
    - name: Comment PR with results
      if: github.event_name == 'pull_request'
      uses: actions/github-script@v7
      with:
        script: |
          const fs = require('fs');
          const resultsTable = fs.readFileSync('docs/benchmark-results.md', 'utf8');
          
          // Create comment body
          const body = `## 📊 Performance Benchmark Results
          
          ${resultsTable}
          
          <details>
          <summary>View Charts</summary>
          
          See the workflow artifacts for throughput and latency comparison charts.
          
          </details>
          
          ---
          *Benchmarked on: \`${{ runner.os }}\` with \`${{ runner.arch }}\`*
          *Commit: \`${{ github.sha }}\`*`;
          
          github.rest.issues.createComment({
            issue_number: context.issue.number,
            owner: context.repo.owner,
            repo: context.repo.repo,
            body: body
          });
    
    - name: Prepare files for GitHub Pages deployment
      if: |
        (github.ref == 'refs/heads/master' && github.event_name == 'push') ||
        (github.event_name == 'pull_request_target' && github.event.action == 'closed' && github.event.pull_request.merged == true)
      run: |
        echo "🚀 Preparing GitHub Pages deployment..."
        echo "📁 Current working directory: $(pwd)"
        echo "📊 docs/ directory contents before deployment:"
        ls -la docs/
        echo "🔍 Generated files verification:"
        echo "  index.html exists: $(test -f docs/index.html && echo 'YES' || echo 'NO')"
        echo "  gil-fairness.html exists: $(test -f docs/gil-fairness.html && echo 'YES' || echo 'NO')"
        echo "  benchmark-results.json exists: $(test -f docs/benchmark-results.json && echo 'YES' || echo 'NO')"
        echo "📋 Detailed file listing with sizes:"
        find docs/ -type f -exec ls -lh {} \; 2>/dev/null || echo "No files found in docs/"
        
        echo ""
        echo "🔧 Creating deployment staging area (ignoring .gitignore)..."
        # Create a temporary staging area for deployment
        mkdir -p docs-deploy
        
        # Copy all files from docs/ to docs-deploy/, including gitignored files
        cp -r docs/* docs-deploy/ 2>/dev/null || true
        
        echo "📁 Deployment staging area contents:"
        ls -la docs-deploy/
        echo "📏 File count in deployment area: $(find docs-deploy/ -type f | wc -l)"
        echo "💾 Total deployment size: $(du -sh docs-deploy/ | cut -f1)"
      
    - name: Deploy to GitHub Pages
      if: |
        (github.ref == 'refs/heads/master' && github.event_name == 'push') ||
        (github.event_name == 'pull_request_target' && github.event.action == 'closed' && github.event.pull_request.merged == true)
      uses: peaceiris/actions-gh-pages@v4
      with:
        github_token: ${{ secrets.GITHUB_TOKEN }}
        publish_dir: ./docs-deploy       # Use staging area that bypasses .gitignore
        keep_files: false               # Always replace entire content
        allow_empty_commit: true        # Force deployment even if content appears unchanged
        force_orphan: true              # Force orphan gh-pages branch (clean history)
        publish_branch: gh-pages        # Target branch
        enable_jekyll: false            # Disable Jekyll processing
        exclude_assets: '.github'       # Don't include .github folder in deployment

  compare-baseline:
    name: Compare with Baseline
    runs-on: ubuntu-latest
    if: github.event_name == 'pull_request'
    
    steps:
    - uses: actions/checkout@v4
      with:
        fetch-depth: 0
    
    - name: Install uv
      uses: astral-sh/setup-uv@v5
    
    - name: Install dependencies
      run: |
        sudo apt-get update
        sudo apt-get install -y build-essential cmake
        cd scripts
        uv sync
    
    # Run benchmarks on PR branch
    - name: Build and benchmark PR
      run: |
        cd test && make clean && make all CFLAGS="-O3 -DNDEBUG"
        cd ..
        BUILD_DIR=test ./scripts/run_benchmarks.sh
        mv docs/benchmark-results.json /tmp/pr-results.json
    
    # Checkout and run benchmarks on base branch
    - name: Checkout base branch
      run: |
        # Clean any generated files (not tracked in git)
        rm -rf docs/benchmark-results.* docs/index.html docs/*-comparison.png || true
        git fetch origin ${{ github.base_ref }}
        git checkout origin/${{ github.base_ref }}
    
    - name: Build and benchmark base
      id: base_benchmark
      continue-on-error: true
      run: |
        echo "Attempting to build and benchmark base branch..."
        if cd test && make clean && make all CFLAGS="-O3 -DNDEBUG" 2>/dev/null; then
          cd ..
          if BUILD_DIR=test ./scripts/run_benchmarks.sh 2>/dev/null; then
            mv docs/benchmark-results.json /tmp/base-results.json
            echo "base_success=true" >> $GITHUB_OUTPUT
          else
            echo "⚠️ Base branch benchmark execution failed - benchmark infrastructure may be incompatible"
            echo "base_success=false" >> $GITHUB_OUTPUT
          fi
        else
          echo "⚠️ Base branch build failed - benchmark infrastructure may be missing"
          echo "base_success=false" >> $GITHUB_OUTPUT
        fi
    
    - name: Compare results
      run: |
        # Create a robust comparison script that handles missing/incompatible base data
        cat > compare.py << 'PYEOF'
        import json
        import sys
        import os
        
        def safe_load_results(path):
            try:
                with open(path) as f:
                    data = json.load(f)
                    # Validate structure
                    if isinstance(data, list) and len(data) > 0:
                        return {r['benchmark'] + '_' + r['implementation']: r for r in data}
                    return {}
            except (FileNotFoundError, json.JSONDecodeError, KeyError) as e:
                print(f"⚠️ Could not load {path}: {e}")
                return {}
        
        def main():
            # Load results with error handling
            base = safe_load_results('/tmp/base-results.json')
            pr = safe_load_results('/tmp/pr-results.json')
            
            if not base:
                print("## 📊 Performance Benchmarks (Base Branch Unavailable)\n")
                print("⚠️ **Base branch comparison unavailable** - benchmark infrastructure may be incompatible\n")
                print("### PR Branch Results Summary\n")
                
                if pr:
                    for key in sorted(pr.keys()):
                        pr_throughput = pr[key]['results']['overall'].get('throughput_items_per_sec', 0)
                        print(f"✅ **{key}**: {pr_throughput:,.0f} items/sec")
                else:
                    print("❌ No valid benchmark results available")
                return
            
            print("## 🔬 Performance Comparison vs Base Branch\n")
            
            compared_any = False
            for key in sorted(pr.keys()):
                if key not in base:
                    print(f"🆕 **{key}**: New benchmark (no baseline)")
                    continue
                
                pr_throughput = pr[key]['results']['overall'].get('throughput_items_per_sec', 0)
                base_throughput = base[key]['results']['overall'].get('throughput_items_per_sec', 0)
                
                if base_throughput > 0:
                    change = ((pr_throughput / base_throughput) - 1) * 100
                    emoji = "🟢" if change > 2 else "🔴" if change < -2 else "⚪"
                    print(f"{emoji} **{key}**: {change:+.1f}% ({pr_throughput:,.0f} vs {base_throughput:,.0f} items/sec)")
                    compared_any = True
            
            if not compared_any:
                print("⚠️ No benchmarks could be compared - likely incompatible result formats")
        
        if __name__ == "__main__":
            main()
        PYEOF
        
        cd scripts
        uv run python ../compare.py > /tmp/comparison.md
        cat /tmp/comparison.md
    
    - name: Comment comparison
      uses: actions/github-script@v7
      with:
        script: |
          const fs = require('fs');
          const comparison = fs.readFileSync('/tmp/comparison.md', 'utf8');
          
          github.rest.issues.createComment({
            issue_number: context.issue.number,
            owner: context.repo.owner,
            repo: context.repo.repo,
            body: `## 🔬 Performance Regression Check\n\n${comparison}\n\n---\n*Changes > ±2% are highlighted*`
          });
