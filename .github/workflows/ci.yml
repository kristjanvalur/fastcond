name: CI

on:
  push:
    branches: [ master, main, develop ]
  pull_request:
    branches: [ master, main, develop ]

jobs:
  build-and-test:
    name: ${{ matrix.os }} - ${{ matrix.compiler }} - ${{ matrix.build_type }}
    runs-on: ${{ matrix.os }}
    timeout-minutes: 10
    
    strategy:
      fail-fast: false
      matrix:
        os: [ubuntu-latest, macos-latest, windows-latest]
        compiler: [gcc, clang]
        build_type: [Release, Debug]
        exclude:
          # macOS doesn't have gcc (uses Apple Clang)
          - os: macos-latest
            compiler: gcc
          # Windows: use MSVC, not gcc/clang directly
          - os: windows-latest
            compiler: gcc
          - os: windows-latest
            compiler: clang
        include:
          # Add Windows with MSVC
          - os: windows-latest
            compiler: msvc
            build_type: Release
          - os: windows-latest
            compiler: msvc
            build_type: Debug
    
    steps:
    - uses: actions/checkout@v4
    
    - name: Disable man-db (speeds up apt-get)
      if: runner.os == 'Linux'
      run: |
        sudo mkdir -p /etc/dpkg/dpkg.cfg.d
        echo 'path-exclude /usr/share/man/*' | sudo tee /etc/dpkg/dpkg.cfg.d/01_nodoc
    
    - name: Install dependencies (Ubuntu)
      if: runner.os == 'Linux'
      run: |
        sudo apt-get update
        sudo apt-get install -y --no-install-recommends build-essential cmake
    
    - name: Set compiler
      if: runner.os != 'Windows'
      run: |
        if [ "${{ matrix.compiler }}" = "gcc" ]; then
          echo "CC=gcc" >> $GITHUB_ENV
        else
          echo "CC=clang" >> $GITHUB_ENV
        fi
    
    - name: Configure CMake
      run: |
        if [ "${{ runner.os }}" = "Windows" ]; then
          # Windows with MSVC - GIL tests now supported with test_portability.h
          # Other tests (qtest, strongtest) still need pthread, so skip those
          cmake -B build \
                -G "Visual Studio 17 2022" \
                -DFASTCOND_BUILD_TESTS=ON \
                -DFASTCOND_BUILD_BENCHMARKS=OFF
        else
          # Unix-like systems - specify compiler and build type
          cmake -B build \
                -DCMAKE_BUILD_TYPE=${{ matrix.build_type }} \
                -DCMAKE_C_COMPILER=${{ matrix.compiler }} \
                -DFASTCOND_BUILD_TESTS=ON \
                -DFASTCOND_BUILD_BENCHMARKS=ON
        fi
      shell: bash
    
    - name: Build
      run: |
        if [ "${{ runner.os }}" = "Windows" ]; then
          cmake --build build --config ${{ matrix.build_type }} --parallel
        else
          cmake --build build --config ${{ matrix.build_type }} -j$(nproc 2>/dev/null || sysctl -n hw.ncpu)
        fi
      shell: bash
    
    - name: Run tests
      run: |
        cd build
        if [ "${{ runner.os }}" = "Windows" ]; then
          # Windows: Run portable tests with native CONDITION_VARIABLE baseline
          echo "=== Running GIL tests on Windows ==="
          echo "Native CONDITION_VARIABLE:"
          ./${{ matrix.build_type }}/gil_test_native.exe 4 1000
          echo ""
          echo "Fastcond:"
          ./${{ matrix.build_type }}/gil_test_fc.exe 4 1000
          echo ""
          echo "Fairness disabled (native):"
          ./${{ matrix.build_type }}/gil_test_native_unfair.exe 4 1000
          echo ""
          echo "=== Running qtest (producer-consumer) on Windows ==="
          echo "Native CONDITION_VARIABLE:"
          ./${{ matrix.build_type }}/qtest_native.exe 10000 4 10
          echo ""
          echo "Fastcond (strong semantics):"
          ./${{ matrix.build_type }}/qtest_fc.exe 10000 4 10
          echo ""
          echo "=== Running strongtest (single condition) on Windows ==="
          echo "Native CONDITION_VARIABLE:"
          ./${{ matrix.build_type }}/strongtest_native.exe 10000 5
          echo ""
          echo "Fastcond (strong semantics):"
          ./${{ matrix.build_type }}/strongtest_fc.exe 10000 5
          echo ""
          echo "=== Running patch test on Windows ==="
          echo "Patch test:"
          ./${{ matrix.build_type }}/patch_test_cond.exe
        else
          # On POSIX systems, run full test suite via CTest
          ctest --output-on-failure --build-config ${{ matrix.build_type }}
        fi
      shell: bash
    
    - name: Run benchmarks (Release only)
      if: matrix.build_type == 'Release' && runner.os != 'Windows'
      run: ./scripts/benchmark.sh
    
    - name: Run performance benchmarks and collect data (Release only)
      if: matrix.build_type == 'Release'
      run: |
        # Set platform identifiers for CI
        if [ "${{ runner.os }}" = "Linux" ]; then
          export FASTCOND_PLATFORM="linux"
          export FASTCOND_OS_VERSION="ubuntu-latest"
        elif [ "${{ runner.os }}" = "macOS" ]; then
          export FASTCOND_PLATFORM="macos"
          export FASTCOND_OS_VERSION="macos-latest"
        elif [ "${{ runner.os }}" = "Windows" ]; then
          export FASTCOND_PLATFORM="windows"
          export FASTCOND_OS_VERSION="windows-latest"
        fi
        
        # Determine build directory
        if [ "${{ runner.os }}" = "Windows" ]; then
          BUILD_DIR="build/${{ matrix.build_type }}"
        else
          BUILD_DIR="build"
        fi
        
        # Run benchmark script
        ./scripts/run_performance_benchmarks.sh "$BUILD_DIR" "performance-results.csv"
      shell: bash
    
    - name: Upload performance results artifact
      if: matrix.build_type == 'Release'
      uses: actions/upload-artifact@v4
      with:
        name: perf-${{ runner.os }}-${{ matrix.compiler }}
        path: performance-results.csv
        retention-days: 30

  sanitizers:
    name: ${{ matrix.sanitizer }} sanitizer
    runs-on: ubuntu-latest
    timeout-minutes: 15
    
    strategy:
      fail-fast: false
      matrix:
        sanitizer: [address, thread, undefined]
        # AddressSanitizer (ASan): Detects memory errors like buffer overflows,
        #   use-after-free, double-free, and memory leaks
        # ThreadSanitizer (TSan): Detects data races and other threading issues
        #   Critical for this library as it's designed for concurrent use
        # UndefinedBehaviorSanitizer (UBSan): Detects undefined behavior like
        #   integer overflow, null pointer dereference, misaligned access
    
    steps:
    - uses: actions/checkout@v4
    
    - name: Disable man-db (speeds up apt-get)
      run: |
        sudo mkdir -p /etc/dpkg/dpkg.cfg.d
        echo 'path-exclude /usr/share/man/*' | sudo tee /etc/dpkg/dpkg.cfg.d/01_nodoc
    
    - name: Install dependencies
      run: |
        sudo apt-get update
        sudo apt-get install -y --no-install-recommends clang cmake
    
    - name: Configure with sanitizer
      run: |
        cmake -B build-${{ matrix.sanitizer }} \
              -DCMAKE_C_COMPILER=clang \
              -DCMAKE_BUILD_TYPE=Debug \
              -DCMAKE_C_FLAGS="-fsanitize=${{ matrix.sanitizer }} -fno-omit-frame-pointer" \
              -DFASTCOND_BUILD_TESTS=ON
    
    - name: Build
      run: cmake --build build-${{ matrix.sanitizer }} -j$(nproc)
    
    - name: Run tests with sanitizer
      run: |
        cd build-${{ matrix.sanitizer }}
        ctest --output-on-failure
      env:
        ASAN_OPTIONS: detect_leaks=1:check_initialization_order=1
        UBSAN_OPTIONS: print_stacktrace=1

  static-analysis:
    name: Static Analysis
    runs-on: ubuntu-latest
    timeout-minutes: 10
    
    steps:
    - uses: actions/checkout@v4
    
    - name: Disable man-db (speeds up apt-get)
      run: |
        sudo mkdir -p /etc/dpkg/dpkg.cfg.d
        echo 'path-exclude /usr/share/man/*' | sudo tee /etc/dpkg/dpkg.cfg.d/01_nodoc
    
    - name: Install clang-tidy
      run: |
        sudo apt-get update
        sudo apt-get install -y --no-install-recommends clang-tidy cmake
    
    - name: Run clang-tidy
      run: |
        cmake -B build -DCMAKE_EXPORT_COMPILE_COMMANDS=ON
        clang-tidy fastcond/*.c -- -I./fastcond

  format-check:
    name: Code Formatting Check
    runs-on: ubuntu-latest
    timeout-minutes: 5
    
    steps:
    - uses: actions/checkout@v4
    
    - name: Disable man-db (speeds up apt-get)
      run: |
        sudo mkdir -p /etc/dpkg/dpkg.cfg.d
        echo 'path-exclude /usr/share/man/*' | sudo tee /etc/dpkg/dpkg.cfg.d/01_nodoc
    
    - name: Install clang-format
      run: |
        sudo apt-get update
        sudo apt-get install -y --no-install-recommends clang-format
    
    - name: Install uv
      uses: astral-sh/setup-uv@v5
      with:
        enable-cache: true
    
    - name: Check C code formatting
      run: |
        find fastcond test -name "*.c" -o -name "*.h" | xargs clang-format -n -Werror
    
    - name: Check Python code formatting
      run: |
        uvx ruff format --check scripts/
    
    - name: Lint Python code
      run: |
        uvx ruff check scripts/

  analyze-and-deploy-performance:
    name: Analyze Cross-Platform Performance
    runs-on: ubuntu-latest
    timeout-minutes: 10
    needs: [build-and-test]
    if: github.ref == 'refs/heads/master' && github.event_name == 'push'
    
    steps:
    - uses: actions/checkout@v4
    
    - name: Install uv
      uses: astral-sh/setup-uv@v5
      with:
        enable-cache: true
    
    - name: Download all performance artifacts
      uses: actions/download-artifact@v4
      with:
        pattern: perf-*
        path: artifacts
    
    - name: List downloaded artifacts
      run: |
        echo "=== Downloaded artifacts structure ==="
        find artifacts -type f -name "*.csv" -exec echo "Found: {}" \;
        echo ""
        echo "=== Artifact directory contents ==="
        ls -lah artifacts/
    
    - name: Merge performance data
      run: |
        # Combine all CSV files, preserving header from first file
        first_file=true
        for csv_file in artifacts/*/performance-results.csv; do
          if [ "$first_file" = true ]; then
            cat "$csv_file" > combined-performance.csv
            first_file=false
          else
            # Skip header line for subsequent files
            tail -n +2 "$csv_file" >> combined-performance.csv
          fi
        done
        
        echo "=== Combined performance data ==="
        wc -l combined-performance.csv
        head -20 combined-performance.csv
    
    - name: Analyze performance data
      run: |
        mkdir -p performance-output
        cd scripts
        uv sync
        uv run python analyze_performance.py \
          ../combined-performance.csv \
          --output-dir ../performance-output
    
    - name: Create performance index page
      run: |
        - name: Create cross-platform index page
      run: |
        GENERATED_DATE=$(date -u +"%Y-%m-%d %H:%M:%S UTC")
        cat > performance-output/index.html << EOF
        <!DOCTYPE html>
        <html lang="en">
        <head>
            <meta charset="UTF-8">
            <meta name="viewport" content="width=device-width, initial-scale=1.0">
            <title>fastcond Performance Benchmarks</title>
            <style>
                body {
                    font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, sans-serif;
                    max-width: 1200px;
                    margin: 0 auto;
                    padding: 20px;
                    line-height: 1.6;
                    color: #333;
                }
                h1 {
                    color: #2c3e50;
                    border-bottom: 3px solid #3498db;
                    padding-bottom: 10px;
                }
                h2 {
                    color: #34495e;
                    margin-top: 30px;
                }
                img {
                    max-width: 100%;
                    height: auto;
                    margin: 20px 0;
                    box-shadow: 0 2px 8px rgba(0,0,0,0.1);
                    border-radius: 4px;
                }
                .chart-container {
                    margin: 30px 0;
                }
                table {
                    border-collapse: collapse;
                    width: 100%;
                    margin: 20px 0;
                }
                th, td {
                    border: 1px solid #ddd;
                    padding: 12px;
                    text-align: left;
                }
                th {
                    background-color: #3498db;
                    color: white;
                    font-weight: 600;
                }
                tr:nth-child(even) {
                    background-color: #f8f9fa;
                }
                .footer {
                    margin-top: 50px;
                    padding-top: 20px;
                    border-top: 1px solid #ddd;
                    color: #7f8c8d;
                    font-size: 0.9em;
                }
                a {
                    color: #3498db;
                    text-decoration: none;
                }
                a:hover {
                    text-decoration: underline;
                }
            </style>
        </head>
        <body>
            <h1>🚀 fastcond Cross-Platform Performance Benchmarks</h1>
            <p>Performance comparison of fastcond synchronization primitives vs native implementations across Linux, macOS, and Windows platforms.</p>
            
            <h2>📊 qtest - Producer-Consumer Queue Test</h2>
            <p>Tests producer-consumer patterns using separate not_empty/not_full conditions with 4 threads and queue size 10.</p>
            
            <div class="chart-container">
                <h3>Speedup vs Native</h3>
                <img src="qtest-speedup.png" alt="qtest Speedup Comparison">
                <p><em>Values > 1.0 indicate fastcond outperforms native implementation on that platform.</em></p>
            </div>
            
            <div class="chart-container">
                <h3>Absolute Throughput</h3>
                <img src="qtest-throughput.png" alt="qtest Throughput Comparison">
            </div>
            
            <h2>📊 strongtest - Single Condition Variable Test</h2>
            <p>Validates strong semantics using a single condition variable for both producer and consumer threads.</p>
            
            <div class="chart-container">
                <h3>Speedup vs Native</h3>
                <img src="strongtest-speedup.png" alt="strongtest Speedup Comparison">
                <p><em>This test specifically validates that only already-waiting threads are woken up.</em></p>
            </div>
            
            <div class="chart-container">
                <h3>Absolute Throughput</h3>
                <img src="strongtest-throughput.png" alt="strongtest Throughput Comparison">
            </div>
            
            <h2>📊 gil_test - GIL Contention Simulation</h2>
            <p>Simulates Python's Global Interpreter Lock with high contention across 4 threads and 1,000 acquisitions.</p>
            
            <div class="chart-container">
                <h3>Speedup vs Native</h3>
                <img src="gil_test-speedup.png" alt="gil_test Speedup Comparison">
                <p><em>GIL patterns have different performance characteristics due to high lock contention.</em></p>
            </div>
            
            <div class="chart-container">
                <h3>Absolute Throughput</h3>
                <img src="gil_test-throughput.png" alt="gil_test Throughput Comparison">
            </div>
            </div>
            
            <h2>📈 Detailed Results</h2>
            <ul>
                <li><a href="performance-comparison.md">performance-comparison.md</a> - Detailed benchmark results table</li>
                <li><a href="performance-summary.json">performance-summary.json</a> - Raw data in JSON format</li>
            </ul>
            
            <h2>📝 About These Benchmarks</h2>
            <p>These benchmarks are automatically collected from CI builds across multiple platforms:</p>
            <ul>
                <li><strong>Linux (ubuntu-latest)</strong>: gcc and clang compilers</li>
                <li><strong>macOS (macos-latest)</strong>: clang compiler</li>
                <li><strong>Windows (windows-latest)</strong>: MSVC compiler</li>
            </ul>
            
            <p>Tests include:</p>
            <ul>
                <li><strong>qtest</strong>: Producer-consumer queue test using separate not_empty/not_full conditions</li>
                <li><strong>strongtest</strong>: Single condition variable test validating strong semantics</li>
                <li><strong>gil_test</strong>: GIL (Global Interpreter Lock) contention simulation</li>
            </ul>
            
            <div class="footer">
                <p>Generated: ${GENERATED_DATE}</p>
                <p>Repository: <a href="https://github.com/kristjanvalur/fastcond">kristjanvalur/fastcond</a></p>
                <p>Documentation: <a href="https://github.com/kristjanvalur/fastcond#readme">README</a></p>
            </div>
        </body>
        </html>
        EOF
    
    - name: List generated files
      run: |
        echo "=== Generated performance files ==="
        ls -lah performance-output/
        echo ""
        echo "=== File contents check ==="
        for file in performance-output/*; do
          echo "📄 $file: $(wc -l < "$file" 2>/dev/null || echo "binary") lines"
        done
    
    - name: Remove .gitignore from output to prevent blocking deployment
      run: |
        # Remove any .gitignore that might block PNG/HTML files on gh-pages
        rm -f performance-output/.gitignore
    
    - name: Deploy to GitHub Pages
      uses: peaceiris/actions-gh-pages@v4
      with:
        github_token: ${{ secrets.GITHUB_TOKEN }}
        publish_dir: ./performance-output
        destination_dir: performance
        keep_files: true
        enable_jekyll: false
        force_orphan: false
        exclude_assets: '.github,.gitignore'
    
    - name: Configure git for historical database commits
      run: |
        git config --global user.email "github-actions[bot]@users.noreply.github.com"
        git config --global user.name "GitHub Actions Bot"
    
    - name: Store platform results in historical database
      run: |
        echo "📊 Storing cross-platform results in historical database..."
        # Convert performance-summary.json to benchmark results format
        cd scripts
        uv run python store_benchmark_history.py \
          ../performance-output/performance-summary.json \
          --history-type platform \
          --run-id "${{ github.run_id }}"
