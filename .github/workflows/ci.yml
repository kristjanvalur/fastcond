name: CI

on:
  push:
    branches: [ master, main, develop ]
  pull_request:
    branches: [ master, main, develop ]

jobs:
  build-and-test:
    name: ${{ matrix.os }} - ${{ matrix.compiler }} - ${{ matrix.build_type }}
    runs-on: ${{ matrix.os }}
    timeout-minutes: 10
    
    strategy:
      fail-fast: false
      matrix:
        os: [ubuntu-latest, macos-13, windows-latest]
        compiler: [gcc, clang]
        build_type: [Release, Debug]
        exclude:
          # macOS doesn't have gcc (uses Apple Clang)
          - os: macos-13
            compiler: gcc
          # Windows: use MSVC, not gcc/clang directly
          - os: windows-latest
            compiler: gcc
          - os: windows-latest
            compiler: clang
        include:
          # Add Windows with MSVC
          - os: windows-latest
            compiler: msvc
            build_type: Release
          - os: windows-latest
            compiler: msvc
            build_type: Debug
    
    steps:
    - uses: actions/checkout@v4
    
    - name: Disable man-db (speeds up apt-get)
      if: runner.os == 'Linux'
      run: |
        sudo mkdir -p /etc/dpkg/dpkg.cfg.d
        echo 'path-exclude /usr/share/man/*' | sudo tee /etc/dpkg/dpkg.cfg.d/01_nodoc
    
    - name: Install dependencies (Ubuntu)
      if: runner.os == 'Linux'
      run: |
        sudo apt-get update
        sudo apt-get install -y --no-install-recommends build-essential cmake
    
    - name: Set compiler
      if: runner.os != 'Windows'
      run: |
        if [ "${{ matrix.compiler }}" = "gcc" ]; then
          echo "CC=gcc" >> $GITHUB_ENV
        else
          echo "CC=clang" >> $GITHUB_ENV
        fi
    
    - name: Configure CMake
      run: |
        if [ "${{ runner.os }}" = "Windows" ]; then
          # Windows with MSVC - All tests now supported with test_portability.h
          cmake -B build \
                -G "Visual Studio 17 2022" \
                -DFASTCOND_BUILD_TESTS=ON \
                -DFASTCOND_BUILD_BENCHMARKS=ON
        else
          # Unix-like systems - specify compiler and build type
          cmake -B build \
                -DCMAKE_BUILD_TYPE=${{ matrix.build_type }} \
                -DCMAKE_C_COMPILER=${{ matrix.compiler }} \
                -DFASTCOND_BUILD_TESTS=ON \
                -DFASTCOND_BUILD_BENCHMARKS=ON
        fi
      shell: bash
    
    - name: Build
      run: |
        if [ "${{ runner.os }}" = "Windows" ]; then
          cmake --build build --config ${{ matrix.build_type }} --parallel
        else
          cmake --build build --config ${{ matrix.build_type }} -j$(nproc 2>/dev/null || sysctl -n hw.ncpu)
        fi
      shell: bash
    
    - name: Run tests
      run: |
        cd build
        if [ "${{ runner.os }}" = "Windows" ]; then
          # Windows: Run portable tests with native CONDITION_VARIABLE baseline
          echo "=== Running GIL tests on Windows ==="
          echo "Native CONDITION_VARIABLE:"
          ./${{ matrix.build_type }}/gil_test_native.exe 4 1000
          echo ""
          echo "Fastcond:"
          ./${{ matrix.build_type }}/gil_test_fc.exe 4 1000
          echo ""
          echo "Fairness disabled (native):"
          ./${{ matrix.build_type }}/gil_test_native_unfair.exe 4 1000
          echo ""
          echo "=== Running qtest (producer-consumer) on Windows ==="
          echo "Native CONDITION_VARIABLE:"
          ./${{ matrix.build_type }}/qtest_native.exe 10000 4 10
          echo ""
          echo "Fastcond (strong semantics):"
          ./${{ matrix.build_type }}/qtest_fc.exe 10000 4 10
          echo ""
          echo "=== Running strongtest (single condition) on Windows ==="
          echo "Native CONDITION_VARIABLE:"
          ./${{ matrix.build_type }}/strongtest_native.exe 10000 5
          echo ""
          echo "Fastcond (strong semantics):"
          ./${{ matrix.build_type }}/strongtest_fc.exe 10000 5
          echo ""
          echo "=== Running patch test on Windows ==="
          echo "Patch test:"
          ./${{ matrix.build_type }}/patch_test_cond.exe
        else
          # On POSIX systems, run full test suite via CTest
          ctest --output-on-failure --build-config ${{ matrix.build_type }}
        fi
      shell: bash
    
    - name: Run benchmarks (Release only)
      if: matrix.build_type == 'Release' && runner.os != 'Windows'
      run: ./scripts/benchmark.sh
    
    - name: Run performance benchmarks and collect data (Release only)
      if: matrix.build_type == 'Release'
      run: |
        # Set platform identifiers for CI
        if [ "${{ runner.os }}" = "Linux" ]; then
          export FASTCOND_PLATFORM="linux"
          export FASTCOND_OS_VERSION="ubuntu-latest"
        elif [ "${{ runner.os }}" = "macOS" ]; then
          export FASTCOND_PLATFORM="macos"
          export FASTCOND_OS_VERSION="macos-13"
        elif [ "${{ runner.os }}" = "Windows" ]; then
          export FASTCOND_PLATFORM="windows"
          export FASTCOND_OS_VERSION="windows-latest"
        fi
        
        # Determine build directory
        if [ "${{ runner.os }}" = "Windows" ]; then
          BUILD_DIR="build/${{ matrix.build_type }}"
        else
          BUILD_DIR="build"
        fi
        
        # Run robust benchmark harness with statistical analysis
        # Uses 15 iterations + 1 warmup for excellent IQR outlier detection
        # Reduced items per run for more statistical samples (15 runs √ó 33K ‚âà same CI time as 5 √ó 100K)
        # IQR filtering will automatically reject any remaining slow runs
        python3 scripts/benchmark_runner.py "$BUILD_DIR" \
          --output-csv performance-results.csv \
          --iterations 15 \
          --warmup 1 \
          --items 33000 \
          --timeout 60 \
          --platform "$FASTCOND_PLATFORM" \
          --os-version "$FASTCOND_OS_VERSION"
      shell: bash
    
    - name: Upload performance results artifact
      if: matrix.build_type == 'Release'
      uses: actions/upload-artifact@v4
      with:
        name: perf-${{ runner.os }}-${{ matrix.compiler }}
        path: performance-results.csv
        retention-days: 30

  sanitizers:
    name: ${{ matrix.sanitizer }} sanitizer
    runs-on: ubuntu-latest
    timeout-minutes: 15
    
    strategy:
      fail-fast: false
      matrix:
        sanitizer: [address, thread, undefined]
        # AddressSanitizer (ASan): Detects memory errors like buffer overflows,
        #   use-after-free, double-free, and memory leaks
        # ThreadSanitizer (TSan): Detects data races and other threading issues
        #   Critical for this library as it's designed for concurrent use
        # UndefinedBehaviorSanitizer (UBSan): Detects undefined behavior like
        #   integer overflow, null pointer dereference, misaligned access
    
    steps:
    - uses: actions/checkout@v4
    
    - name: Disable man-db (speeds up apt-get)
      run: |
        sudo mkdir -p /etc/dpkg/dpkg.cfg.d
        echo 'path-exclude /usr/share/man/*' | sudo tee /etc/dpkg/dpkg.cfg.d/01_nodoc
    
    - name: Install dependencies
      run: |
        sudo apt-get update
        sudo apt-get install -y --no-install-recommends clang cmake
    
    - name: Configure with sanitizer
      run: |
        cmake -B build-${{ matrix.sanitizer }} \
              -DCMAKE_C_COMPILER=clang \
              -DCMAKE_BUILD_TYPE=Debug \
              -DCMAKE_C_FLAGS="-fsanitize=${{ matrix.sanitizer }} -fno-omit-frame-pointer" \
              -DFASTCOND_BUILD_TESTS=ON
    
    - name: Build
      run: cmake --build build-${{ matrix.sanitizer }} -j$(nproc)
    
    - name: Run tests with sanitizer
      run: |
        cd build-${{ matrix.sanitizer }}
        ctest --output-on-failure
      env:
        ASAN_OPTIONS: detect_leaks=1:check_initialization_order=1
        UBSAN_OPTIONS: print_stacktrace=1

  static-analysis:
    name: Static Analysis
    runs-on: ubuntu-latest
    timeout-minutes: 10
    
    steps:
    - uses: actions/checkout@v4
    
    - name: Disable man-db (speeds up apt-get)
      run: |
        sudo mkdir -p /etc/dpkg/dpkg.cfg.d
        echo 'path-exclude /usr/share/man/*' | sudo tee /etc/dpkg/dpkg.cfg.d/01_nodoc
    
    - name: Install clang-tidy
      run: |
        sudo apt-get update
        sudo apt-get install -y --no-install-recommends clang-tidy cmake
    
    - name: Run clang-tidy
      run: |
        cmake -B build -DCMAKE_EXPORT_COMPILE_COMMANDS=ON
        clang-tidy fastcond/*.c -- -I./fastcond

  format-check:
    name: Code Formatting Check
    runs-on: ubuntu-latest
    timeout-minutes: 5
    
    steps:
    - uses: actions/checkout@v4
    
    - name: Disable man-db (speeds up apt-get)
      run: |
        sudo mkdir -p /etc/dpkg/dpkg.cfg.d
        echo 'path-exclude /usr/share/man/*' | sudo tee /etc/dpkg/dpkg.cfg.d/01_nodoc
    
    - name: Install clang-format
      run: |
        sudo apt-get update
        sudo apt-get install -y --no-install-recommends clang-format
    
    - name: Install uv
      uses: astral-sh/setup-uv@v5
      with:
        enable-cache: true
    
    - name: Check C code formatting
      run: |
        find fastcond test -name "*.c" -o -name "*.h" | xargs clang-format -n -Werror
    
    - name: Check Python code formatting
      run: |
        uvx ruff format --check scripts/
    
    - name: Lint Python code
      run: |
        uvx ruff check scripts/

  analyze-and-deploy-performance:
    name: Analyze Cross-Platform Performance
    runs-on: ubuntu-latest
    timeout-minutes: 10
    needs: [build-and-test]
    if: github.ref == 'refs/heads/master' && github.event_name == 'push'
    
    steps:
    - uses: actions/checkout@v4
    
    - name: Install uv
      uses: astral-sh/setup-uv@v5
      with:
        enable-cache: true
    
    - name: Download all performance artifacts
      uses: actions/download-artifact@v4
      with:
        pattern: perf-*
        path: artifacts
    
    - name: List downloaded artifacts
      run: |
        echo "=== Downloaded artifacts structure ==="
        find artifacts -type f -name "*.csv" -exec echo "Found: {}" \;
        echo ""
        echo "=== Artifact directory contents ==="
        ls -lah artifacts/
    
    - name: Merge performance data
      run: |
        # Combine all CSV files, preserving header from first file
        first_file=true
        for csv_file in artifacts/*/performance-results.csv; do
          if [ "$first_file" = true ]; then
            cat "$csv_file" > combined-performance.csv
            first_file=false
          else
            # Skip header line for subsequent files
            tail -n +2 "$csv_file" >> combined-performance.csv
          fi
        done
        
        echo "=== Combined performance data ==="
        wc -l combined-performance.csv
        head -20 combined-performance.csv
    
    - name: Analyze performance data
      run: |
        mkdir -p performance-output
        cd scripts
        uv sync
        uv run python analyze_performance.py \
          ../combined-performance.csv \
          --output-dir ../performance-output
    
    - name: Create cross-platform index page
      run: |
        GENERATED_DATE=$(date -u +"%Y-%m-%d %H:%M:%S UTC")
        cat > performance-output/index.html << EOF
        <!DOCTYPE html>
        <html lang="en">
        <head>
            <meta charset="UTF-8">
            <meta name="viewport" content="width=device-width, initial-scale=1.0">
            <title>fastcond Performance Benchmarks</title>
            <style>
                body {
                    font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, sans-serif;
                    max-width: 1200px;
                    margin: 0 auto;
                    padding: 20px;
                    line-height: 1.6;
                    color: #333;
                }
                h1 {
                    color: #2c3e50;
                    border-bottom: 3px solid #3498db;
                    padding-bottom: 10px;
                }
                h2 {
                    color: #34495e;
                    margin-top: 30px;
                }
                img {
                    max-width: 100%;
                    height: auto;
                    margin: 20px 0;
                    box-shadow: 0 2px 8px rgba(0,0,0,0.1);
                    border-radius: 4px;
                }
                .chart-container {
                    margin: 30px 0;
                }
                table {
                    border-collapse: collapse;
                    width: 100%;
                    margin: 20px 0;
                }
                th, td {
                    border: 1px solid #ddd;
                    padding: 12px;
                    text-align: left;
                }
                th {
                    background-color: #3498db;
                    color: white;
                    font-weight: 600;
                }
                tr:nth-child(even) {
                    background-color: #f8f9fa;
                }
                .footer {
                    margin-top: 50px;
                    padding-top: 20px;
                    border-top: 1px solid #ddd;
                    color: #7f8c8d;
                    font-size: 0.9em;
                }
                a {
                    color: #3498db;
                    text-decoration: none;
                }
                a:hover {
                    text-decoration: underline;
                }
            </style>
        </head>
        <body>
            <h1>üöÄ fastcond Cross-Platform Performance Benchmarks</h1>
            <p>Performance comparison of fastcond synchronization primitives vs native implementations across Linux, macOS, and Windows platforms.</p>
            
            <h2>üìä qtest - Producer-Consumer Queue Test</h2>
            <p>Tests producer-consumer patterns using separate not_empty/not_full conditions with 4 threads and queue size 10.</p>
            
            <div class="chart-container">
                <h3>Speedup vs Native</h3>
                <img src="qtest-speedup.png" alt="qtest Speedup Comparison">
                <p><em>Values > 1.0 indicate fastcond outperforms native implementation on that platform.</em></p>
            </div>
            
            <div class="chart-container">
                <h3>Absolute Throughput</h3>
                <img src="qtest-throughput.png" alt="qtest Throughput Comparison">
            </div>
            
            <h2>üìä strongtest - Single Condition Variable Test</h2>
            <p>Validates strong semantics using a single condition variable for both producer and consumer threads.</p>
            
            <div class="chart-container">
                <h3>Speedup vs Native</h3>
                <img src="strongtest-speedup.png" alt="strongtest Speedup Comparison">
                <p><em>This test specifically validates that only already-waiting threads are woken up.</em></p>
            </div>
            
            <div class="chart-container">
                <h3>Absolute Throughput</h3>
                <img src="strongtest-throughput.png" alt="strongtest Throughput Comparison">
            </div>
            
            <h2>üìä gil_test - GIL Contention Simulation</h2>
            <p>Simulates Python's Global Interpreter Lock with high contention across 4 threads and 1,000 acquisitions.</p>
            
            <div class="chart-container">
                <h3>Speedup vs Native</h3>
                <img src="gil_test-speedup.png" alt="gil_test Speedup Comparison">
                <p><em>GIL patterns have different performance characteristics due to high lock contention.</em></p>
            </div>
            
            <div class="chart-container">
                <h3>Absolute Throughput</h3>
                <img src="gil_test-throughput.png" alt="gil_test Throughput Comparison">
            </div>
            </div>
            
            <h2>üìà Detailed Results</h2>
            <ul>
                <li><a href="performance-comparison.md">performance-comparison.md</a> - Detailed benchmark results table</li>
                <li><a href="performance-summary.json">performance-summary.json</a> - Raw data in JSON format</li>
            </ul>
            
            <h2>üìù About These Benchmarks</h2>
            <p>These benchmarks are automatically collected from CI builds across multiple platforms:</p>
            <ul>
                <li><strong>Linux (ubuntu-latest)</strong>: gcc and clang compilers</li>
                <li><strong>macOS (macos-latest)</strong>: clang compiler</li>
                <li><strong>Windows (windows-latest)</strong>: MSVC compiler</li>
            </ul>
            
            <p>Tests include:</p>
            <ul>
                <li><strong>qtest</strong>: Producer-consumer queue test using separate not_empty/not_full conditions</li>
                <li><strong>strongtest</strong>: Single condition variable test validating strong semantics</li>
                <li><strong>gil_test</strong>: GIL (Global Interpreter Lock) contention simulation</li>
            </ul>
            
            <div class="footer">
                <p>Generated: ${GENERATED_DATE}</p>
                <p>Repository: <a href="https://github.com/kristjanvalur/fastcond">kristjanvalur/fastcond</a></p>
                <p>Documentation: <a href="https://github.com/kristjanvalur/fastcond#readme">README</a></p>
            </div>
        </body>
        </html>
        EOF
    
    - name: List generated files
      run: |
        echo "=== Generated performance files ==="
        ls -lah performance-output/
        echo ""
        echo "=== File contents check ==="
        for file in performance-output/*; do
          echo "üìÑ $file: $(wc -l < "$file" 2>/dev/null || echo "binary") lines"
        done
    
    - name: Remove .gitignore from output to prevent blocking deployment
      run: |
        # Remove any .gitignore that might block PNG/HTML files on gh-pages
        rm -f performance-output/.gitignore
    
    - name: Create updated root index page with performance summary
      run: |
        # Extract key performance metrics from the cross-platform data
        cd scripts
        uv run python -c "
        import json
        import datetime
        
        # Load the performance summary
        with open('../performance-output/performance-summary.json', 'r') as f:
            data = json.load(f)
        
        # Calculate overall performance improvements
        improvements = []
        platforms = set()
        
        for result in data:
            platform = result['platform']
            test = result['test']
            speedup = result['speedup_vs_native']
            platforms.add(platform.title())
            
            if speedup > 1.0:
                improvements.append(f'{test}: {speedup:.1f}x faster on {platform.title()}')
        
        platforms_str = ', '.join(sorted(platforms))
        avg_improvement = sum(r['speedup_vs_native'] for r in data if r['speedup_vs_native'] > 1.0)
        num_improvements = len([r for r in data if r['speedup_vs_native'] > 1.0])
        avg_improvement = avg_improvement / max(num_improvements, 1)
        
        best_improvement = max((r['speedup_vs_native'] for r in data), default=1.0)
        
        # Generate timestamp
        timestamp = datetime.datetime.utcnow().strftime('%Y-%m-%d %H:%M:%S UTC')
        
        # Create root index.html
        html_content = f'''<!DOCTYPE html>
<html lang=\"en\">
<head>
    <meta charset=\"UTF-8\">
    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">
    <title>fastcond - Fast POSIX Condition Variables</title>
    <style>
        body {{
            font-family: -apple-system, BlinkMacSystemFont, \"Segoe UI\", Roboto, sans-serif;
            max-width: 1200px;
            margin: 0 auto;
            padding: 20px;
            line-height: 1.6;
            color: #333;
        }}
        .header {{
            text-align: center;
            background: linear-gradient(135deg, #2c3e50 0%, #3498db 100%);
            color: white;
            padding: 40px 20px;
            border-radius: 12px;
            margin-bottom: 30px;
            box-shadow: 0 4px 6px rgba(0,0,0,0.1);
        }}
        .header h1 {{
            margin: 0;
            font-size: 2.5em;
            font-weight: 300;
        }}
        .header .subtitle {{
            margin: 10px 0 0 0;
            font-size: 1.2em;
            opacity: 0.9;
        }}
        .performance-highlight {{
            background: linear-gradient(135deg, #27ae60, #2ecc71);
            color: white;
            padding: 30px;
            border-radius: 12px;
            margin: 30px 0;
            text-align: center;
            box-shadow: 0 4px 6px rgba(0,0,0,0.1);
        }}
        .performance-highlight h2 {{
            margin: 0 0 15px 0;
            font-size: 1.8em;
        }}
        .performance-highlight .metric {{
            font-size: 2.2em;
            font-weight: bold;
            margin: 10px 0;
        }}
        .performance-highlight .details {{
            font-size: 1.1em;
            opacity: 0.9;
        }}
        .features-grid {{
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(300px, 1fr));
            gap: 20px;
            margin: 30px 0;
        }}
        .feature-card {{
            background: white;
            padding: 25px;
            border-radius: 8px;
            box-shadow: 0 2px 4px rgba(0,0,0,0.1);
            border-left: 4px solid #3498db;
        }}
        .feature-card h3 {{
            margin: 0 0 15px 0;
            color: #2c3e50;
        }}
        .cta-section {{
            background: #f8f9fa;
            padding: 30px;
            border-radius: 12px;
            text-align: center;
            margin: 30px 0;
        }}
        .cta-buttons {{
            margin: 20px 0 0 0;
        }}
        .cta-buttons a {{
            display: inline-block;
            margin: 0 10px;
            padding: 12px 24px;
            background: #3498db;
            color: white;
            text-decoration: none;
            border-radius: 6px;
            font-weight: 500;
            transition: background 0.3s;
        }}
        .cta-buttons a:hover {{
            background: #2980b9;
        }}
        .cta-buttons a.primary {{
            background: #27ae60;
        }}
        .cta-buttons a.primary:hover {{
            background: #229954;
        }}
        .footer {{
            margin-top: 50px;
            padding-top: 20px;
            border-top: 1px solid #ddd;
            color: #7f8c8d;
            text-align: center;
            font-size: 0.9em;
        }}
    </style>
</head>
<body>
    <div class=\"header\">
        <h1>‚ö° fastcond</h1>
        <div class=\"subtitle\">Fast POSIX Condition Variables Using Only Semaphores</div>
    </div>
    
    <div class=\"performance-highlight\">
        <h2>üöÄ Proven Performance Across Platforms</h2>
        <div class=\"metric\">Up to {best_improvement:.1f}x Faster</div>
        <div class=\"details\">
            Comprehensive benchmarks across {platforms_str} show consistent performance improvements.<br>
            Average improvement: {avg_improvement:.1f}x faster than native implementations.
        </div>
    </div>
    
    <div class=\"features-grid\">
        <div class=\"feature-card\">
            <h3>üéØ Drop-in Replacement</h3>
            <p>Full POSIX condition variable semantics with zero API changes. Replace pthread_cond_* with fastcond equivalents and get immediate performance benefits.</p>
        </div>
        
        <div class=\"feature-card\">
            <h3>üèóÔ∏è Two Variants</h3>
            <p><strong>Weak</strong>: Maximum performance with relaxed semantics. <strong>Strong</strong>: Full POSIX compliance with excellent performance.</p>
        </div>
        
        <div class=\"feature-card\">
            <h3>üåç Cross-Platform</h3>
            <p>Tested and optimized for Linux, macOS, and Windows. Uses platform-appropriate semaphore implementations for best performance.</p>
        </div>
        
        <div class=\"feature-card\">
            <h3>üìä Rigorously Tested</h3>
            <p>Comprehensive test suite with AddressSanitizer, ThreadSanitizer, and statistical analysis. Every commit is benchmarked across platforms.</p>
        </div>
    </div>
    
    <div class=\"cta-section\">
        <h2>Get Started with fastcond</h2>
        <p>Ready to speed up your thread synchronization? Choose your path:</p>
        <div class=\"cta-buttons\">
            <a href=\"performance/\" class=\"primary\">üìä View Detailed Benchmarks</a>
            <a href=\"https://github.com/kristjanvalur/fastcond\">‚≠ê Star on GitHub</a>
            <a href=\"https://github.com/kristjanvalur/fastcond/releases\">üì¶ Download Release</a>
            <a href=\"https://github.com/kristjanvalur/fastcond#readme\">üìñ Documentation</a>
        </div>
    </div>
    
    <div class=\"footer\">
        <p>Built with semaphores and Nordic engineering precision.</p>
        <p>Last updated: {timestamp}</p>
        <p><a href=\"https://github.com/kristjanvalur/fastcond\">kristjanvalur/fastcond</a></p>
    </div>
</body>
</html>'''
        
        # Write the HTML file
        with open('../performance-output/root-index.html', 'w') as f:
            f.write(html_content)
        
        print('‚úÖ Generated updated root index page')
        "
    
    - name: Prepare complete GitHub Pages deployment
      run: |
        # Create a deployment directory with both root page and performance subdir
        mkdir -p gh-pages-deploy/performance
        
        # Copy performance analysis to subdirectory
        cp -r performance-output/* gh-pages-deploy/performance/
        
        # Copy the generated root index as the main index.html
        cp performance-output/root-index.html gh-pages-deploy/index.html
        
        echo "=== Deployment structure ==="
        find gh-pages-deploy -type f | head -20
    
    - name: Deploy to GitHub Pages
      uses: peaceiris/actions-gh-pages@v4
      with:
        github_token: ${{ secrets.GITHUB_TOKEN }}
        publish_dir: ./gh-pages-deploy
        destination_dir: .
        keep_files: false  # Replace old content with new structure
        enable_jekyll: false
        force_orphan: false
        exclude_assets: '.github,.gitignore'
    
    - name: Configure git for historical database commits
      run: |
        git config --global user.email "github-actions[bot]@users.noreply.github.com"
        git config --global user.name "GitHub Actions Bot"
    
    - name: Store platform results in historical database
      run: |
        echo "üìä Storing cross-platform results in historical database..."
        # Convert performance-summary.json to benchmark results format
        cd scripts
        uv run python store_benchmark_history.py \
          ../performance-output/performance-summary.json \
          --history-type platform \
          --run-id "${{ github.run_id }}"
